from utils.document_loader import CSVLoader, MDLoader, PDFLoader, TXTLoader, DocxLoader, HTMLLoader
from utils.load_config import configs
from utils.base_func import parse_response
from typing import List, Dict
from pathlib import Path
import numpy as np
from tqdm import tqdm
from FlagEmbedding import FlagModel, FlagReranker
import numpy as np
import json
import os
import time

class Rag:
    def __init__(self):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        - æ”¯æŒå¤šç§æ–‡æ¡£åŠ è½½å™¨
        - è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„æ–‡æ¡£å’Œå‘é‡
        - æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£
        """
        self.config = configs
        self.documents_path = Path(self.config['rag']['document_path'])
        self.files = self.get_all_files_in_directory()
        print('documents files:', self.files)
        self.vector_store_path = Path(self.config['rag']['vector_store']['index_path'])
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¶è‡ªåŠ¨åŠ è½½å·²æœ‰æ•°æ®
        self.docs = self._load_metadata()
        self.doc_vectors = self._load_vectors()

        self.device = self.config['rag']['embedding_model']['device']
        self.embedding_model = FlagModel(self.config['rag']['embedding_model']['path'], 
                  query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                  use_fp16=True,devices=self.device)
        self.embedding_model.dimension = self.config['rag']['embedding_model']['dimension']
        
        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        self.reranker_device = self.config['rag']['reranker_model']['device']
        self.reranker_model = FlagReranker(
            self.config['rag']['reranker_model']['path'],
            use_fp16=True,
            device=self.reranker_device
        )
        
        # æ£€ç´¢å‚æ•°
        self.top_k = self.config['rag']['retrieval']['top_k']
        self.initial_retrieval_k = self.config['rag']['retrieval']['initial_retrieval_k']
        self.score_threshold = self.config['rag']['retrieval']['score_threshold']
        self.rerank_threshold = self.config['rag']['retrieval']['rerank_score_threshold']
        self.similarity_metric = self.config['rag']['vector_store']['similarity_metric']
    
    def get_all_files_in_directory(self) -> List[str]:
        directory_path = Path(self.documents_path)  # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ª Path å¯¹è±¡
        return [str(file) for file in directory_path.rglob('*') if file.is_file()]

    def _get_vector_path(self) -> Path:
        """è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "doc_vectors.npy"

    def _get_metadata_path(self) -> Path:
        """è·å–å…ƒæ•°æ®å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "metadata.json"

    def _load_vectors(self) -> np.ndarray:
        """åŠ è½½å·²ä¿å­˜çš„å‘é‡"""
        vector_path = self._get_vector_path()
        if vector_path.exists():
            return np.load(vector_path)
        return None

    def _load_metadata(self) -> List[Dict]:
        """åŠ è½½å·²ä¿å­˜çš„å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path()
        if metadata_path.exists():
            with open(metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def _save_data(self,vectors=True,docs=True):
        """ä¿å­˜å½“å‰æ•°æ®åˆ°ç£ç›˜"""
        # ä¿å­˜å‘é‡
        if vectors and self.doc_vectors is not None:
            np.save(self._get_vector_path(), self.doc_vectors)
        
        # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰æ–°çš„å…ƒæ•°æ®æ ¼å¼å­—æ®µ
        if docs:
            for doc in self.docs:
                if 'chunk_index' not in doc:
                    doc['chunk_index'] = 0
                if 'chunk_content' not in doc:
                    doc['chunk_content'] = doc.get('content', '')
            if 'total_chunks' not in doc:
                # è®¡ç®—æ­¤æ–‡ä»¶çš„æ€»å—æ•°
                file_path = doc.get('file_path')
                same_file_docs = [d for d in self.docs if d.get('file_path') == file_path]
                doc['total_chunks'] = len(same_file_docs)
            # ä¿å­˜å…ƒæ•°æ®
            with open(self._get_metadata_path(), "w", encoding="utf-8") as f:
                json.dump(self.docs, f, ensure_ascii=False, indent=2)
    
    def _cosine_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - å‘é‡åŒ–è®¡ç®—
        
        ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        cos(Î¸) = (AÂ·B) / (|A|Â·|B|)
        å…¶ä¸­,Aå’ŒBæ˜¯ä¸¤ä¸ªå‘é‡,AÂ·Bæ˜¯å®ƒä»¬çš„ç‚¹ç§¯,|A|å’Œ|B|æ˜¯å®ƒä»¬çš„æ¨¡ã€‚
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # è®¡ç®—ç‚¹ç§¯
        dot_products = np.dot(query_vector, doc_vectors.T).flatten()
        
        # è®¡ç®—æ¨¡
        query_norm = np.linalg.norm(query_vector)
        doc_norms = np.linalg.norm(doc_vectors, axis=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = dot_products / (query_norm * doc_norms)
        
        return similarities
    
    def _l2_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—L2ç›¸ä¼¼åº¦ï¼ˆæ¬§æ°è·ç¦»ï¼‰- å‘é‡åŒ–è®¡ç®—
        
        L2ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        d(A,B) = sqrt(sum((A_i - B_i)^2))
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        """
        # è®¡ç®—æ¬§æ°è·ç¦»
        # ä½¿ç”¨å¹¿æ’­æœºåˆ¶ï¼š(1, dim) - (num_docs, dim) => (num_docs, dim)
        distances = np.linalg.norm(query_vector - doc_vectors, axis=1)
        
        # ç”±äºæ¬§æ°è·ç¦»è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œä¸ºäº†ä¿æŒä¸ä½™å¼¦ç›¸ä¼¼åº¦ä¸€è‡´ï¼ˆå€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
        # æˆ‘ä»¬å¯ä»¥å¯¹è·ç¦»å–è´Ÿå€¼æˆ–å€’æ•°ï¼Œè¿™é‡Œé€‰æ‹©å–è´Ÿå€¼
        similarities = -distances
        
        return similarities
        
    def load_documents(self, file_paths: List[str]) -> None:
        """
        åŠ è½½å¤šç§æ ¼å¼çš„æ–‡æ¡£
        """
        # è½¬æ¢æ‰€æœ‰è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
        abs_paths = [str(Path(fp).absolute()) for fp in file_paths]
        
        # è¿‡æ»¤å·²åŠ è½½æ–‡ä»¶ï¼ˆä½¿ç”¨é›†åˆæé«˜æŸ¥è¯¢æ•ˆç‡ï¼‰
        existing_files = {doc["file_path"] for doc in self.docs}
        new_files = [fp for fp in abs_paths if fp not in existing_files]
        
        if not new_files:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²åŠ è½½è¿‡")
            return

        try:
            # å°†numpyæ•°ç»„è½¬ä¸ºåˆ—è¡¨ä¾¿äºè¿½åŠ 
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = []

            with tqdm(new_files, desc="ğŸ“ æ€»ä½“è¿›åº¦") as global_pbar:
                for file_path in global_pbar:
                    global_pbar.set_postfix_str(f'å¤„ç†ä¸­: {Path(file_path).name}')
                    
                    try:
                        # è·å–æ–‡ä»¶åŠ è½½å™¨
                        if file_path.endswith('.csv'):
                            loader = CSVLoader()
                        elif file_path.endswith('.docx'):
                            loader = DocxLoader()
                        elif file_path.endswith('.md'):
                            loader = MDLoader()
                        elif file_path.endswith('.pdf'):
                            loader = PDFLoader()
                        elif file_path.endswith('.html') or file_path.endswith('.htm'):
                            loader = HTMLLoader()
                        elif (file_path.endswith('.txt') or (Path(file_path).suffix == '' and Path(file_path).is_file())):
                            loader = TXTLoader()
                        else:
                            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

                        # åŠ è½½æ–‡æ¡£å†…å®¹
                        chunks = loader.load(file_path)
                        total_chunks = len(chunks)
                        
                        # å¤„ç†æ–‡æ¡£å—
                        file_vectors = []
                        for i, chunk in enumerate(tqdm(chunks, desc=f"ğŸ“„ {Path(file_path).name}", leave=True)):
                            # å­˜å‚¨å…ƒæ•°æ® - ä½¿ç”¨æ–°çš„æ ¼å¼
                            self.docs.append({
                                "file_path": file_path,
                                "chunk_index": i,  # è®°å½•å—ç´¢å¼•
                                "chunk_content": str(chunk),  # æ–°å¢å­—æ®µä»¥ç¬¦åˆæ ¼å¼è¦æ±‚
                                "total_chunks": total_chunks,  # è®°å½•æ€»å—æ•°
                                "timestamp": time.time()  # æ·»åŠ æ—¶é—´æˆ³ç”¨äºç‰ˆæœ¬æ§åˆ¶
                            })
                            # ç”Ÿæˆå‘é‡
                            file_vectors.append(self.embedding_model.encode(str(chunk)))
                        # è¿½åŠ å‘é‡
                        vectors.extend(file_vectors)

                    except Exception as e:
                        print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                        continue

            # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„
            self.doc_vectors = np.array(vectors)
            
        finally:
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_data()

    def reset(self):
        """é‡ç½®æ‰€æœ‰å­˜å‚¨æ•°æ®"""
        self.doc_vectors = None
        self.docs = []
        if self._get_vector_path().exists():
            os.remove(self._get_vector_path())
        if self._get_metadata_path().exists():
            os.remove(self._get_metadata_path())
    
    def rebuild_vector_db(self, file_path=None, chunk_indices=None):
        """
        é‡å»ºå‘é‡æ•°æ®åº“
        åœ¨ä¿®æ”¹äº†å…ƒæ•°æ®ä¸­çš„æ–‡æ¡£å†…å®¹åè°ƒç”¨æ­¤æ–¹æ³•æ¥æ›´æ–°å‘é‡
        
        å‚æ•°:
            file_path: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡å»ºçš„æ–‡ä»¶è·¯å¾„
            chunk_indices: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡å»ºçš„chunkç´¢å¼•åˆ—è¡¨(å½“file_pathæä¾›æ—¶æœ‰æ•ˆ)
        """
        if not self.docs:
            print("æ— å¯ç”¨æ–‡æ¡£ï¼Œæ— æ³•é‡å»ºå‘é‡åº“")
            return
        
        try:
            # å¦‚æœå·²æœ‰å‘é‡å­˜å‚¨ï¼ŒåŠ è½½å®ƒ
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = [None] * len(self.docs)
            
            # ç¡®å®šéœ€è¦é‡å»ºçš„æ–‡æ¡£ç´¢å¼•
            rebuild_indices = []
            if file_path is not None:
                # åªé‡å»ºæŒ‡å®šæ–‡ä»¶çš„æŒ‡å®šå—
                file_path = os.path.abspath(file_path)
                for i, doc in enumerate(self.docs):
                    if doc.get('file_path') == file_path:
                        if chunk_indices is None or int(doc.get('chunk_index', 0)) in chunk_indices:
                            rebuild_indices.append(i)
                print(f"å°†é‡å»ºæ–‡ä»¶ {file_path} çš„ {len(rebuild_indices)} ä¸ªåˆ†å—å‘é‡")
            else:
                # é‡å»ºæ‰€æœ‰æ–‡æ¡£
                rebuild_indices = list(range(len(self.docs)))
                print(f"å°†é‡å»ºæ‰€æœ‰ {len(rebuild_indices)} ä¸ªåˆ†å—å‘é‡")
            
            if not rebuild_indices:
                print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡å»ºçš„æ–‡æ¡£")
                return
                
            print("æ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“...")
            
            with tqdm(rebuild_indices, desc="ğŸ“ é‡å»ºå‘é‡è¿›åº¦") as pbar:
                for i in pbar:
                    doc = self.docs[i]
                    chunk_content = doc.get('chunk_content', '')
                    
                    if chunk_content:
                        # ç”Ÿæˆæ–°çš„å‘é‡
                        vector = self.embedding_model.encode(chunk_content)
                        vectors[i] = vector
                    else:
                        print(f"è­¦å‘Š: æ–‡æ¡£ {doc.get('file_path')} æ²¡æœ‰å†…å®¹ï¼Œè·³è¿‡")
                        # æ·»åŠ ä¸€ä¸ªç©ºå‘é‡ä»¥ä¿æŒç´¢å¼•å¯¹é½
                        vectors[i] = np.zeros(self.embedding_model.dimension)
            
            # æ›´æ–°å‘é‡å­˜å‚¨
            self.doc_vectors = np.array(vectors)
            
            # ä¿å­˜åˆ°ç£ç›˜
            self._save_data()
            
            print(f"å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼Œæ›´æ–°äº† {len(rebuild_indices)} ä¸ªæ–‡æ¡£å‘é‡")
        
        except Exception as e:
            print(f"é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            raise
    
    def retrieve_documents(self, query: str, top_k: int = None, threshold: float = None, rerank_threshold: float = None) -> List[Dict]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå…ˆä½¿ç”¨embeddingæ¨¡å‹æ£€ç´¢ï¼Œå†ä½¿ç”¨rerankeræ¨¡å‹é‡æ’åº
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®å€¼
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
            
        è¿”å›:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        """
        if top_k is None:
            top_k = self.top_k
            
        if threshold is None:
            threshold = self.score_threshold
        
        if rerank_threshold is None:
            rerank_threshold = self.rerank_threshold
        
        if not self.docs or self.doc_vectors is None:
            print("æ— å¯ç”¨æ–‡æ¡£")
            return []

        # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨embeddingæ¨¡å‹è¿›è¡Œåˆæ­¥æ£€ç´¢
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode(query).reshape(1, -1)  # ç¡®ä¿å½¢çŠ¶æ˜¯ (1, dim)
        
        # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
        if self.similarity_metric == 'cosine':
            similarities = self._cosine_similarity(query_vector, self.doc_vectors)
        elif self.similarity_metric == 'l2':
            similarities = self._l2_similarity(query_vector, self.doc_vectors)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡ï¼š{self.similarity_metric}")

        # æ‰¾åˆ°ç›¸ä¼¼åº¦å¾—åˆ†æœ€é«˜çš„initial_retrieval_kä¸ªæ–‡æ¡£
        initial_k = self.initial_retrieval_k
        indices = np.argsort(similarities)[::-1][:initial_k]
        # æ”¶é›†åˆæ­¥æ£€ç´¢ç»“æœ
        initial_results = []
        for idx in indices:
            if similarities[idx] >= threshold:  # ä»ç„¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                doc = dict(self.docs[idx])  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
                doc['initial_score'] = float(similarities[idx])  # ä¿å­˜åˆå§‹å¾—åˆ†
                
                # ç¡®ä¿æ–‡æ¡£å…·æœ‰æ‰€éœ€çš„å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                if 'chunk_index' not in doc:
                    doc['chunk_index'] = 0
                if 'chunk_content' not in doc:
                    doc['chunk_content'] = doc.get('chunk_content', '')
                if 'total_chunks' not in doc:
                    # è®¡ç®—æ­¤æ–‡ä»¶çš„æ€»å—æ•°
                    same_file_docs = [d for d in self.docs if d.get('file_path') == doc.get('file_path')]
                    doc['total_chunks'] = len(same_file_docs)
                    
                initial_results.append(doc)
        
        if not initial_results:
            print("åˆæ­¥æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return []
            
        # ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨rerankeræ¨¡å‹è¿›è¡Œé‡æ’åº
        pairs = []
        for doc in initial_results:
            pairs.append((query, doc['chunk_content']))
        
        # è°ƒç”¨rerankeræ¨¡å‹è·å–é‡æ’åºåˆ†æ•°
        rerank_scores = self.reranker_model.compute_score(pairs)
        
        # å°†rerankeråˆ†æ•°å½’ä¸€åŒ–åˆ°0-1èŒƒå›´å†…ï¼ˆä½¿ç”¨sigmoidå‡½æ•°ï¼‰
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        
        # ä¸ºæ–‡æ¡£æ·»åŠ é‡æ’åºåˆ†æ•°ï¼ˆå½’ä¸€åŒ–åçš„ï¼‰
        reranked_results = []
        for i, doc in enumerate(initial_results):
            # å¤åˆ¶æ–‡æ¡£ä»¥ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µ
            reranked_doc = dict(doc)
            # ä½¿ç”¨sigmoidå°†å¾—åˆ†å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            normalized_score = float(sigmoid(rerank_scores[i]))
            reranked_doc['score'] = normalized_score  # rerankerå½’ä¸€åŒ–åˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
            if reranked_doc['score'] >= rerank_threshold:
                reranked_results.append(reranked_doc)
        
        # æŒ‰é‡æ’åºåˆ†æ•°é™åºæ’åº
        reranked_results = sorted(reranked_results, key=lambda x: x['score'], reverse=True)
        
        # è¿”å›top_kä¸ªæ–‡æ¡£
        final_results = reranked_results[:top_k]
        
        return final_results
    
    def generate_prompt(self, query: str, top_k: int = None, threshold: float = None, rerank_threshold: float = None, is_image: bool = False) -> str:
        """
        ç”Ÿæˆ RAG æç¤º
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param top_k: è¿”å›æœ€ç›¸å…³çš„ top_k æ–‡æ¡£
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :param rerank_threshold: é‡æ’åºåˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :param is_image: æ˜¯å¦ä¸ºå›¾ç‰‡æŸ¥è¯¢
        :return: ç”Ÿæˆçš„æç¤ºæ–‡æœ¬
        """
        if top_k is None:
            top_k = self.top_k  
        if threshold is None:
            threshold = self.score_threshold
        if rerank_threshold is None:
            rerank_threshold = self.rerank_threshold
        relevant_docs = self.retrieve_documents(query, top_k, threshold, rerank_threshold)
        
        if is_image:
            # å›¾ç‰‡æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿
            prompt = f"""
            è¯·æ ¹æ®OCRè¯†åˆ«ç»“æœå’Œç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·å…³äºå›¾ç‰‡çš„é—®é¢˜ã€‚

            å›¾ç‰‡å†…å®¹å’Œç”¨æˆ·æé—®: {query}

            ç›¸å…³æ–‡æ¡£:\n
            """
        else:
            # å¸¸è§„æ–‡æœ¬æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿
            prompt = f"""
            è¯·æ ¹æ®ç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜ã€‚è‹¥æœ‰çš„æ–‡æ¡£ä¸ç›¸å…³ï¼Œå°½é‡ä¸è¦è¾“å‡ºä¸ä¸ç›¸å…³æ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ ¹æ®ä½ è‡ªå·±æ¥è¾“å‡ºã€‚

            ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜: {query}

            ç›¸å…³æ–‡æ¡£:\n
            """
            
        for i, doc in enumerate(relevant_docs):
            prompt += f"""
            æ–‡æ¡£ {i+1} [æ¥è‡ª: {doc['file_path']}]:
            {doc['chunk_content']}
            ç›¸ä¼¼åº¦å¾—åˆ†: {doc['score']:.4f}\n\n
            """
        
        if is_image:
            prompt += "è¯·åˆ†æå›¾ç‰‡OCRè¯†åˆ«çš„å†…å®¹ï¼Œå¹¶ç»“åˆç›¸å…³æ–‡æ¡£æä¾›å‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸å›¾ç‰‡å†…å®¹æ— å…³ï¼Œè¯·ä¼˜å…ˆåŸºäºå›¾ç‰‡å†…å®¹å›ç­”ã€‚"
        else:
            prompt += "è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä¸å­˜åœ¨äºæ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚"
            
        return prompt