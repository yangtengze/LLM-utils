from utils.document_loader import CSVLoader, MDLoader, PDFLoader, TXTLoader, DocxLoader, HTMLLoader
from utils.load_config import configs
from utils.base_func import call_language_model, remove_think_tag
from typing import List, Dict, Tuple, Optional, Union, Any
from pathlib import Path
import numpy as np
from tqdm import tqdm
from FlagEmbedding import FlagModel, FlagReranker
import json
import os
import time
import re
from io import StringIO

class Rag:
    def __init__(self):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        - æ”¯æŒå¤šç§æ–‡æ¡£åŠ è½½å™¨
        - è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„æ–‡æ¡£å’Œå‘é‡
        - æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£
        """
        self.config = configs
        self.documents_path = Path(self.config['rag']['document_path'])
        self.files = self.get_all_files_in_directory()
        print('documents files:', self.files)
        self.vector_store_path = Path(self.config['rag']['vector_store']['index_path'])
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¶è‡ªåŠ¨åŠ è½½å·²æœ‰æ•°æ®
        self.docs = self._load_metadata()
        self.doc_vectors = self._load_vectors()

        self.device = self.config['rag']['embedding_model']['device']
        self.embedding_model = FlagModel(self.config['rag']['embedding_model']['path'], 
                  query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                  use_fp16=True,devices=self.device)
        self.embedding_model.dimension = self.config['rag']['embedding_model']['dimension']
        
        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        self.reranker_device = self.config['rag']['reranker_model']['device']
        self.reranker_model = FlagReranker(
            self.config['rag']['reranker_model']['path'],
            use_fp16=True,
            device=self.reranker_device
        )
        
        # æ£€ç´¢å‚æ•°
        self.top_k = self.config['rag']['retrieval']['top_k']
        self.initial_retrieval_k = self.config['rag']['retrieval']['initial_retrieval_k']
        self.score_threshold = self.config['rag']['retrieval']['score_threshold']
        self.rerank_threshold = self.config['rag']['retrieval']['rerank_score_threshold']
        self.similarity_metric = self.config['rag']['vector_store']['similarity_metric']
        
        # æ·»åŠ ç»“æœç¼“å­˜
        self.retrieval_cache = {}
        self.cache_max_size = 50  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        self.cache_ttl = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰

    def _generate_chunk_summary(self, chunk_content: str, max_length: int = 150) -> str:
        """ç”Ÿæˆæ–‡æ¡£å—çš„æ‘˜è¦"""
        prompt = f"""
        ç›´æ¥å¼€å§‹ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œä»…åˆ—æ ¸å¿ƒè¦ç‚¹ï¼šé¦–å…ˆæ˜¯æ€»ç»“å‡ºæ¥çš„æ ‡é¢˜ï¼Œå†ç”¨ç¬¦å·ã€Œâ€¢ã€åˆ†é¡¹ï¼Œæœ€åç”¨ç¬¦å·ã€Œâ†’ã€æ€»ç»“ã€‚é¿å…ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚(ä¸è¶…è¿‡{max_length}ä¸ªå­—ç¬¦ï¼‰
          
        æ–‡æœ¬å†…å®¹:
        {chunk_content}
        """
        summary = call_language_model(prompt)
        summary = remove_think_tag(summary)
        return summary
    
    def _query_enhance(self, query: str) -> str:
        """
        å¯¹æŸ¥è¯¢è¿›è¡Œå¢å¼º
        """
        # å¦‚æœæŸ¥è¯¢å¾ˆçŸ­æˆ–ä¸åŒ…å«æ•°å­¦è¡¨è¾¾å¼ï¼Œè·³è¿‡å¢å¼º
        if len(query) < 20 and not any(char in query for char in "+-*/^()={}[]"):
            return query
        
        # ç¼“å­˜å¢å¼ºæŸ¥è¯¢ç»“æœ
        cache_key = f"enhance_{query}"
        if cache_key in self.retrieval_cache:
            cache_entry = self.retrieval_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                return cache_entry['results']
            else:
                del self.retrieval_cache[cache_key]
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œå¢å¼º - åªæœ‰åœ¨å¯èƒ½åŒ…å«æ•°å­¦å…¬å¼æ—¶æ‰è°ƒç”¨LLM
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æµ‹å¯èƒ½çš„æ•°å­¦è¡¨è¾¾å¼
        math_pattern = r'[\+\-\*\/\^\(\)\=\{\}\[\]]|[0-9]+[a-zA-Z]+|[a-zA-Z]+[0-9]+'
        if re.search(math_pattern, query):
            Systemprompt = """
            ä½ æ˜¯ä¸€ä¸ªèƒ½æŠŠæ–‡æœ¬å˜ä¸ºå¸¦æœ‰latexå…¬å¼çš„æ–‡æœ¬çš„ä¸“å®¶ï¼Œä½ ä¸éœ€è¦è§£æ•°å­¦é¢˜ï¼Œåªéœ€è¿›è¡Œè½¬åŒ–ã€‚
            æŠŠä¸‹é¢ç”¨æˆ·æå‡ºçš„é—®é¢˜ä¸­å¸¦æœ‰æ•°å­¦å…¬å¼çš„éƒ¨åˆ†è½¬åŒ–æˆå¸¦æœ‰latexå…¬å¼çš„,ä¸¥æ ¼è½¬æ¢ï¼Œä¸è¦å‡ºé”™ï¼Œå¹¶ä¸”èƒ½ç†è§£ç”¨æˆ·çš„è¯­ä¹‰ï¼Œè¯­ä¹‰é‡Œå¸¦æœ‰æ•°å­¦å…¬å¼çš„ä¹Ÿè¦è½¬æ¢ã€‚
            ä¸è¦æœ‰å¤šä½™çš„è¾“å‡ºã€‚ç›´æ¥ç»™æˆ‘è½¬æ¢åçš„æ–‡æœ¬ï¼Œå†æ¬¡å¼ºè°ƒï¼šï¼ˆæ³¨æ„ï¼‰ä½ ä¸éœ€è¦å»åšè¿™ä¸ªæ•°å­¦é¢˜ã€‚
            """
            enhance_query = call_language_model(query, Systemprompt)
            enhance_query = remove_think_tag(enhance_query)
        else:
            # ä¸åŒ…å«æ•°å­¦è¡¨è¾¾å¼ï¼Œç›´æ¥è¿”å›åŸå§‹æŸ¥è¯¢
            enhance_query = query
        
        # å°†å¢å¼ºç»“æœå­˜å…¥ç¼“å­˜
        self.retrieval_cache[cache_key] = {
            'results': enhance_query,
            'timestamp': time.time()
        }
        
        return enhance_query
    def get_all_files_in_directory(self) -> List[str]:
        directory_path = Path(self.documents_path)  # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ª Path å¯¹è±¡
        return [str(file) for file in directory_path.rglob('*') if file.is_file()]

    def _get_vector_path(self) -> Path:
        """è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "doc_vectors.npy"

    def _get_metadata_path(self) -> Path:
        """è·å–å…ƒæ•°æ®å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "metadata.json"

    def _load_vectors(self) -> np.ndarray:
        """åŠ è½½å·²ä¿å­˜çš„å‘é‡"""
        vector_path = self._get_vector_path()
        if vector_path.exists():
            return np.load(vector_path)
        return None

    def _load_metadata(self) -> List[Dict]:
        """åŠ è½½å·²ä¿å­˜çš„å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path()
        if metadata_path.exists():
            with open(metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def _save_data(self,vectors=True,docs=True):
        """ä¿å­˜å½“å‰æ•°æ®åˆ°ç£ç›˜"""
        # ä¿å­˜å‘é‡
        if vectors and self.doc_vectors is not None:
            np.save(self._get_vector_path(), self.doc_vectors)
        
        # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰æ–°çš„å…ƒæ•°æ®æ ¼å¼å­—æ®µ
        if docs:
            for doc in self.docs:
                if 'chunk_index' not in doc:
                    doc['chunk_index'] = 0
                if 'chunk_content' not in doc:
                    doc['chunk_content'] = doc.get('content', '')
                if 'total_chunks' not in doc:
                    # è®¡ç®—æ­¤æ–‡ä»¶çš„æ€»å—æ•°
                    file_path = doc.get('file_path')
                    same_file_docs = [d for d in self.docs if d.get('file_path') == file_path]
                    doc['total_chunks'] = len(same_file_docs)
                if 'chunk_summary' not in doc:
                    doc['chunk_summary'] = self._generate_chunk_summary(doc['chunk_content'])
            # ä¿å­˜å…ƒæ•°æ®
            with open(self._get_metadata_path(), "w", encoding="utf-8") as f:
                json.dump(self.docs, f, ensure_ascii=False, indent=2)
    
    def _cosine_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - å‘é‡åŒ–è®¡ç®—
        
        ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        cos(Î¸) = (AÂ·B) / (|A|Â·|B|)
        å…¶ä¸­,Aå’ŒBæ˜¯ä¸¤ä¸ªå‘é‡,AÂ·Bæ˜¯å®ƒä»¬çš„ç‚¹ç§¯,|A|å’Œ|B|æ˜¯å®ƒä»¬çš„æ¨¡ã€‚
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # è®¡ç®—ç‚¹ç§¯
        dot_products = np.dot(query_vector, doc_vectors.T).flatten()
        
        # è®¡ç®—æ¨¡
        query_norm = np.linalg.norm(query_vector)
        doc_norms = np.linalg.norm(doc_vectors, axis=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = dot_products / (query_norm * doc_norms)
        
        return similarities
    
    def _l2_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—L2ç›¸ä¼¼åº¦ï¼ˆæ¬§æ°è·ç¦»ï¼‰- å‘é‡åŒ–è®¡ç®—
        
        L2ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        d(A,B) = sqrt(sum((A_i - B_i)^2))
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        """
        # è®¡ç®—æ¬§æ°è·ç¦»
        # ä½¿ç”¨å¹¿æ’­æœºåˆ¶ï¼š(1, dim) - (num_docs, dim) => (num_docs, dim)
        distances = np.linalg.norm(query_vector - doc_vectors, axis=1)
        
        # ç”±äºæ¬§æ°è·ç¦»è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œä¸ºäº†ä¿æŒä¸ä½™å¼¦ç›¸ä¼¼åº¦ä¸€è‡´ï¼ˆå€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
        # æˆ‘ä»¬å¯ä»¥å¯¹è·ç¦»å–è´Ÿå€¼æˆ–å€’æ•°ï¼Œè¿™é‡Œé€‰æ‹©å–è´Ÿå€¼
        similarities = -distances
        
        return similarities
        
    def load_documents(self, file_paths: List[str]) -> None:
        """
        åŠ è½½å¤šç§æ ¼å¼çš„æ–‡æ¡£
        """
        # è½¬æ¢æ‰€æœ‰è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
        abs_paths = [str(Path(fp).absolute()) for fp in file_paths]
        
        # è¿‡æ»¤å·²åŠ è½½æ–‡ä»¶ï¼ˆä½¿ç”¨é›†åˆæé«˜æŸ¥è¯¢æ•ˆç‡ï¼‰
        existing_files = {doc["file_path"] for doc in self.docs}
        new_files = [fp for fp in abs_paths if fp not in existing_files]
        
        if not new_files:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²åŠ è½½è¿‡")
            return

        try:
            # å°†numpyæ•°ç»„è½¬ä¸ºåˆ—è¡¨ä¾¿äºè¿½åŠ 
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = []

            # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
            batch_size = 5  # æ¯æ‰¹å¤„ç†çš„æ–‡ä»¶æ•°é‡
            batches = [new_files[i:i + batch_size] for i in range(0, len(new_files), batch_size)]
            
            all_new_docs = []
            all_new_vectors = []
            
            with tqdm(total=len(new_files), desc="ğŸ“ æ€»ä½“è¿›åº¦") as global_pbar:
                for batch in batches:
                    batch_docs = []
                    batch_vectors = []
                    
                    for file_path in batch:
                        try:
                            # è·å–æ–‡ä»¶åŠ è½½å™¨
                            if file_path.endswith('.csv'):
                                loader = CSVLoader()
                            elif file_path.endswith('.docx'):
                                loader = DocxLoader()
                            elif file_path.endswith('.md'):
                                loader = MDLoader()
                            elif file_path.endswith('.pdf'):
                                loader = PDFLoader()
                            elif file_path.endswith('.html') or file_path.endswith('.htm'):
                                loader = HTMLLoader()
                            elif (file_path.endswith('.txt') or (Path(file_path).suffix == '' and Path(file_path).is_file())):
                                loader = TXTLoader()
                            else:
                                global_pbar.update(1)
                                print(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                                continue

                            # åŠ è½½æ–‡æ¡£å†…å®¹
                            chunks = loader.load(file_path)
                            total_chunks = len(chunks)
                            
                            # ä¼˜åŒ–ï¼šé¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨æ–‡ä»¶å—æ•°é‡
                            if not hasattr(self, '_file_chunks_count'):
                                self._file_chunks_count = {}
                            self._file_chunks_count[file_path] = total_chunks
                            
                            # å¤„ç†æ–‡æ¡£å—
                            file_docs = []
                            file_contents = []
                            
                            # æ”¶é›†å—å†…å®¹ï¼Œå‡†å¤‡æ‰¹é‡ç¼–ç 
                            for i, chunk in enumerate(chunks):
                                chunk = str(chunk)
                                if chunk == '':
                                    continue
                                # ä¼˜åŒ–ï¼šå¹¶è¡Œç”Ÿæˆæ‘˜è¦
                                chunk_summary = self._generate_chunk_summary(chunk)
                                
                                # å­˜å‚¨å…ƒæ•°æ®
                                file_docs.append({
                                    "file_path": file_path,
                                    "chunk_index": i,
                                    "chunk_summary": chunk_summary,
                                    "chunk_content": chunk,
                                    "total_chunks": total_chunks,
                                    "timestamp": time.time(),
                                })
                                
                                # ä¼˜åŒ–ï¼šé¢„å¤„ç†å‘é‡å†…å®¹
                                content_for_vector = f"{chunk_summary}\n{chunk_summary}\n{chunk}"
                                file_contents.append(content_for_vector)
                            
                            # æ‰¹é‡ç¼–ç æ–‡ä»¶å†…å®¹
                            if file_contents:
                                # ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡ç¼–ç è€Œä¸æ˜¯é€ä¸ªç¼–ç 
                                file_vectors = self.embedding_model.encode_corpus(file_contents)
                                batch_vectors.extend(file_vectors)
                                batch_docs.extend(file_docs)
                            
                            global_pbar.update(1)
                            global_pbar.set_postfix_str(f'å¤„ç†å®Œæˆ: {Path(file_path).name}')
                            
                        except Exception as e:
                            global_pbar.update(1)
                            print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                            continue
                    
                    # è¿½åŠ æ–°æ‰¹æ¬¡çš„æ–‡æ¡£å’Œå‘é‡
                    all_new_docs.extend(batch_docs)
                    all_new_vectors.extend(batch_vectors)
                
                # å°†æ‰€æœ‰æ–°æ•°æ®è¿½åŠ åˆ°ç°æœ‰æ•°æ®ä¸­
                self.docs.extend(all_new_docs)
                vectors.extend(all_new_vectors)

                # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„
                self.doc_vectors = np.array(vectors)
                
            print(f"åŠ è½½å®Œæˆï¼Œæ–°å¢ {len(all_new_docs)} ä¸ªæ–‡æ¡£å—")
            
        finally:
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_data()

    def reset(self):
        """é‡ç½®æ‰€æœ‰å­˜å‚¨æ•°æ®"""
        self.doc_vectors = None
        self.docs = []
        if self._get_vector_path().exists():
            os.remove(self._get_vector_path())
        if self._get_metadata_path().exists():
            os.remove(self._get_metadata_path())
    
    def rebuild_vector_db(self, file_path=None, chunk_indices=None):
        """
        é‡å»ºå‘é‡æ•°æ®åº“
        åœ¨ä¿®æ”¹äº†å…ƒæ•°æ®ä¸­çš„æ–‡æ¡£å†…å®¹åè°ƒç”¨æ­¤æ–¹æ³•æ¥æ›´æ–°å‘é‡
        
        å‚æ•°:
            file_path: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡å»ºçš„æ–‡ä»¶è·¯å¾„
            chunk_indices: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡å»ºçš„chunkç´¢å¼•åˆ—è¡¨(å½“file_pathæä¾›æ—¶æœ‰æ•ˆ)
        """
        if not self.docs:
            print("æ— å¯ç”¨æ–‡æ¡£ï¼Œæ— æ³•é‡å»ºå‘é‡åº“")
            return
        
        try:
            # å¦‚æœå·²æœ‰å‘é‡å­˜å‚¨ï¼ŒåŠ è½½å®ƒ
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = [None] * len(self.docs)
            
            # ç¡®å®šéœ€è¦é‡å»ºçš„æ–‡æ¡£ç´¢å¼•
            rebuild_indices = []
            if file_path is not None:
                # åªé‡å»ºæŒ‡å®šæ–‡ä»¶çš„æŒ‡å®šå—
                file_path = os.path.abspath(file_path)
                for i, doc in enumerate(self.docs):
                    if doc.get('file_path') == file_path:
                        if chunk_indices is None or int(doc.get('chunk_index', 0)) in chunk_indices:
                            rebuild_indices.append(i)
                print(f"å°†é‡å»ºæ–‡ä»¶ {file_path} çš„ {len(rebuild_indices)} ä¸ªåˆ†å—å‘é‡")
            else:
                # é‡å»ºæ‰€æœ‰æ–‡æ¡£
                rebuild_indices = list(range(len(self.docs)))
                print(f"å°†é‡å»ºæ‰€æœ‰ {len(rebuild_indices)} ä¸ªåˆ†å—å‘é‡")
            
            if not rebuild_indices:
                print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡å»ºçš„æ–‡æ¡£")
                return
                
            print("æ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“...")
            
            with tqdm(rebuild_indices, desc="ğŸ“ é‡å»ºå‘é‡è¿›åº¦") as pbar:
                for i in pbar:
                    doc = self.docs[i]
                    chunk_content = doc.get('chunk_content', '')
                    chunk_summary = doc.get('chunk_summary', '')
                    # chunk_summary = self._generate_chunk_summary(chunk_content)
                    chunk = f"{chunk_summary}\n{chunk_summary}\n{chunk_content}"
                    if chunk_content:
                        # ç”Ÿæˆæ–°çš„å‘é‡
                        vector = self.embedding_model.encode(chunk)
                        vectors[i] = vector
                    else:
                        print(f"è­¦å‘Š: æ–‡æ¡£ {doc.get('file_path')} æ²¡æœ‰å†…å®¹ï¼Œè·³è¿‡")
                        # æ·»åŠ ä¸€ä¸ªç©ºå‘é‡ä»¥ä¿æŒç´¢å¼•å¯¹é½
                        vectors[i] = np.zeros(self.embedding_model.dimension)
            
            # æ›´æ–°å‘é‡å­˜å‚¨
            self.doc_vectors = np.array(vectors)
            
            # ä¿å­˜åˆ°ç£ç›˜
            self._save_data()
            
            print(f"å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼Œæ›´æ–°äº† {len(rebuild_indices)} ä¸ªæ–‡æ¡£å‘é‡")
        
        except Exception as e:
            print(f"é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            raise
    
    def retrieve_documents(self, query: str, top_k: int = None, threshold: float = None, rerank_threshold: float = None) -> List[Dict]:
        """
        æ£€ç´¢ä¸å¢å¼ºæŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå…ˆä½¿ç”¨embeddingæ¨¡å‹æ£€ç´¢ï¼Œå†ä½¿ç”¨rerankeræ¨¡å‹é‡æ’åº
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®å€¼
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
            rerank_threshold: é‡æ’åºåˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        è¿”å›:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        """
        if top_k is None:
            top_k = self.top_k
            
        if threshold is None:
            threshold = self.score_threshold
        
        if rerank_threshold is None:
            rerank_threshold = self.rerank_threshold
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{query}_{top_k}_{threshold}_{rerank_threshold}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.retrieval_cache:
            cache_entry = self.retrieval_cache[cache_key]
            # æ£€æŸ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                print(f"ä»ç¼“å­˜ä¸­è·å–æ£€ç´¢ç»“æœï¼ŒæŸ¥è¯¢: {query[:30]}...")
                return cache_entry['results']
            else:
                # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤
                del self.retrieval_cache[cache_key]
                
        if not self.docs or self.doc_vectors is None:
            print("æ— å¯ç”¨æ–‡æ¡£")
            return []
        
        # ä¼˜åŒ–æ­¥éª¤1: ä½¿ç”¨batchæ“ä½œç”ŸæˆæŸ¥è¯¢å‘é‡
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode_queries([query]).reshape(1, -1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
        if self.similarity_metric == 'cosine':
            similarities = self._cosine_similarity(query_vector, self.doc_vectors)
        elif self.similarity_metric == 'l2':
            similarities = self._l2_similarity(query_vector, self.doc_vectors)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡ï¼š{self.similarity_metric}")

        # ä¼˜åŒ–æ­¥éª¤2: ä½¿ç”¨å‘é‡åŒ–æ“ä½œæŸ¥æ‰¾topkï¼Œé¿å…å¾ªç¯
        # æ‰¾åˆ°ç›¸ä¼¼åº¦å¾—åˆ†æœ€é«˜çš„initial_retrieval_kä¸ªæ–‡æ¡£
        initial_k = self.initial_retrieval_k
        indices = np.argsort(similarities)[::-1][:initial_k]
        
        # ä¼˜åŒ–æ­¥éª¤3: åˆå§‹è¿‡æ»¤ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
        # è¿‡æ»¤ä½äºé˜ˆå€¼çš„ç´¢å¼•
        mask = similarities[indices] >= threshold
        filtered_indices = indices[mask]
        
        if len(filtered_indices) == 0:
            print("åˆæ­¥æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return []
        
        # ä¼˜åŒ–æ­¥éª¤4: æ‰¹é‡å¤„ç†æ–‡æ¡£
        # æ”¶é›†åˆæ­¥æ£€ç´¢ç»“æœ 
        initial_results = []
        pairs = []  # ä¸ºrerankerå‡†å¤‡çš„å¯¹
        
        for idx in filtered_indices:
            doc = dict(self.docs[idx])  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
            doc['initial_score'] = float(similarities[idx])  # ä¿å­˜åˆå§‹å¾—åˆ†
            
            # ç¡®ä¿æ–‡æ¡£å…·æœ‰æ‰€éœ€çš„å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
            if 'chunk_index' not in doc:
                doc['chunk_index'] = 0
            if 'chunk_content' not in doc:
                doc['chunk_content'] = doc.get('chunk_content', '')
            if 'total_chunks' not in doc:
                # ä¼˜åŒ–: ä½¿ç”¨é¢„è®¡ç®—æˆ–ç¼“å­˜çš„æ€»å—æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'file_path' in doc and hasattr(self, '_file_chunks_count') and doc['file_path'] in self._file_chunks_count:
                    doc['total_chunks'] = self._file_chunks_count[doc['file_path']]
                else:
                    same_file_docs = [d for d in self.docs if d.get('file_path') == doc.get('file_path')]
                    doc['total_chunks'] = len(same_file_docs)
                    # ç¼“å­˜ç»“æœä¾›æœªæ¥ä½¿ç”¨
                    if not hasattr(self, '_file_chunks_count'):
                        self._file_chunks_count = {}
                    self._file_chunks_count[doc['file_path']] = doc['total_chunks']
            
            if 'chunk_summary' not in doc:
                # ä¼˜åŒ–: å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œåˆ›å»ºæ‘˜è¦ä½†å°è¯•é‡ç”¨ä¹‹å‰çš„ç»“æœ
                cache_key_summary = f"summary_{doc.get('file_path')}_{doc.get('chunk_index')}"
                if cache_key_summary in self.retrieval_cache:
                    doc['chunk_summary'] = self.retrieval_cache[cache_key_summary]['results']
                else:
                    doc['chunk_summary'] = self._generate_chunk_summary(doc['chunk_content'])
                    # ç¼“å­˜æ‘˜è¦
                    self.retrieval_cache[cache_key_summary] = {
                        'results': doc['chunk_summary'],
                        'timestamp': time.time()
                    }
            
            initial_results.append(doc)
            
            # å‡†å¤‡ reranker è¾“å…¥
            chunk_content = f"{doc['chunk_summary']}\n{doc['chunk_summary']}\n{doc['chunk_content']}"
            pairs.append((query, chunk_content))
        
        # ä¼˜åŒ–æ­¥éª¤5: æ‰¹é‡è°ƒç”¨rerankerä¸€æ¬¡ï¼Œè€Œä¸æ˜¯åœ¨å¾ªç¯ä¸­é€ä¸ªè°ƒç”¨
        # è°ƒç”¨rerankeræ¨¡å‹è·å–é‡æ’åºåˆ†æ•° - æ‰¹é‡å¤„ç†æ›´é«˜æ•ˆ
        rerank_scores = self.reranker_model.compute_score(pairs)
        
        # å°†rerankeråˆ†æ•°å½’ä¸€åŒ–åˆ°0-1èŒƒå›´å†…ï¼ˆä½¿ç”¨sigmoidå‡½æ•°ï¼‰
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        
        # ä¼˜åŒ–æ­¥éª¤6: ä½¿ç”¨numpyå‘é‡åŒ–æ“ä½œå¤„ç†åˆ†æ•°
        # æ‰¹é‡åº”ç”¨sigmoid
        normalized_scores = sigmoid(np.array(rerank_scores))
        
        # ä¸ºæ–‡æ¡£æ·»åŠ é‡æ’åºåˆ†æ•°ï¼ˆå½’ä¸€åŒ–åçš„ï¼‰
        reranked_results = []
        for i, doc in enumerate(initial_results):
            # å¤åˆ¶æ–‡æ¡£ä»¥ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µ
            reranked_doc = dict(doc)
            # ä½¿ç”¨é¢„è®¡ç®—çš„å½’ä¸€åŒ–åˆ†æ•°
            reranked_doc['score'] = float(normalized_scores[i])
            if reranked_doc['score'] >= rerank_threshold:
                reranked_results.append(reranked_doc)
        
        # æŒ‰é‡æ’åºåˆ†æ•°é™åºæ’åº
        reranked_results = sorted(reranked_results, key=lambda x: x['score'], reverse=True)
        
        # è¿”å›top_kä¸ªæ–‡æ¡£
        final_results = reranked_results[:top_k]
        
        # å°†ç»“æœå­˜å…¥ç¼“å­˜
        self.retrieval_cache[cache_key] = {
            'results': final_results,
            'timestamp': time.time()
        }
        
        # æ¸…ç†ç¼“å­˜
        if len(self.retrieval_cache) > self.cache_max_size:
            # ä¼˜åŒ–æ­¥éª¤7: æ›´é«˜æ•ˆçš„ç¼“å­˜æ¸…ç†
            # æ‰¾å‡ºæœ€æ—§çš„å‡ ä¸ªæ¡ç›®è€Œä¸æ˜¯æ¯æ¬¡åªåˆ é™¤ä¸€ä¸ª
            cache_items = sorted(self.retrieval_cache.items(), key=lambda x: x[1]['timestamp'])
            # åˆ é™¤10%çš„æ—§æ¡ç›®
            items_to_remove = max(1, int(self.cache_max_size * 0.1))
            for i in range(items_to_remove):
                if i < len(cache_items):
                    del self.retrieval_cache[cache_items[i][0]]
        
        # å­˜å‚¨æœ€åä¸€æ¬¡æ£€ç´¢çš„ç»“æœï¼Œä¾¿äºå‰ç«¯è·å–
        self.last_retrieval = {
            'query': query,
            'top_k': top_k,
            'results': final_results,
            'cache_key': cache_key
        }
        
        return final_results
    
    def generate_prompt(self, query: str, top_k: int = None, threshold: float = None, rerank_threshold: float = None, is_image: bool = False) -> str:
        """
        ç”Ÿæˆ RAG æç¤º
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param top_k: è¿”å›æœ€ç›¸å…³çš„ top_k æ–‡æ¡£
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :param rerank_threshold: é‡æ’åºåˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :param is_image: æ˜¯å¦ä¸ºå›¾ç‰‡æŸ¥è¯¢
        :return: ç”Ÿæˆçš„æç¤ºæ–‡æœ¬
        """
        if top_k is None:
            top_k = self.top_k  
        if threshold is None:
            threshold = self.score_threshold
        if rerank_threshold is None:
            rerank_threshold = self.rerank_threshold
            
        # ç”Ÿæˆæœ€æ–°æŸ¥è¯¢çš„ç¼“å­˜é”®
        cache_key = f"prompt_{query}_{top_k}_{threshold}_{rerank_threshold}_{is_image}"
        
        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰æ­¤æç¤º
        if cache_key in self.retrieval_cache:
            cache_entry = self.retrieval_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                print(f"ä»ç¼“å­˜ä¸­è·å–æç¤ºï¼ŒæŸ¥è¯¢: {query[:30]}...")
                return cache_entry['results']
            else:
                del self.retrieval_cache[cache_key]
        
        # ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥é‡ç”¨ä¸Šæ¬¡æ£€ç´¢çš„ç»“æœ
        if hasattr(self, 'last_retrieval') and self.last_retrieval and self.last_retrieval.get('query') == query and self.last_retrieval.get('top_k') >= top_k:
            relevant_docs = self.last_retrieval.get('results', [])[:top_k]
            print(f"é‡ç”¨ä¸Šæ¬¡æ£€ç´¢ç»“æœï¼ŒæŸ¥è¯¢: {query[:30]}...")
        else:
            relevant_docs = self.retrieve_documents(query, top_k, threshold, rerank_threshold)
        
        # æ„å»ºæç¤ºæ¨¡æ¿ - ä½¿ç”¨ StringIO æˆ– StringBuilder æ¨¡å¼æ›´é«˜æ•ˆ
        prompt_builder = StringIO()
        
        if is_image:
            # å›¾ç‰‡æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿
            prompt_builder.write(f"""
è¯·æ ¹æ®OCRè¯†åˆ«ç»“æœå’Œç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·å…³äºå›¾ç‰‡çš„é—®é¢˜ã€‚

å›¾ç‰‡å†…å®¹å’Œç”¨æˆ·æé—®: {query}

ç›¸å…³æ–‡æ¡£:
""")
        else:
            # å¸¸è§„æ–‡æœ¬æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿
            prompt_builder.write(f"""
è¯·æ ¹æ®ç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜ã€‚è‹¥æœ‰çš„æ–‡æ¡£ä¸ç›¸å…³ï¼Œå°½é‡ä¸è¦è¾“å‡ºä¸ä¸ç›¸å…³æ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ ¹æ®ä½ è‡ªå·±æ¥è¾“å‡ºã€‚

ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜: {query}

ç›¸å…³æ–‡æ¡£:
""")
        
        # é«˜æ•ˆæ·»åŠ æ–‡æ¡£å†…å®¹
        for i, doc in enumerate(relevant_docs):
            prompt_builder.write(f"""
æ–‡æ¡£ {i+1} [æ¥è‡ª: {doc['file_path']}]:

[æ‘˜è¦] {doc['chunk_summary']}
[è¡¥å……ç»†èŠ‚] {doc['chunk_content']}

ç›¸ä¼¼åº¦å¾—åˆ†: {doc['score']:.4f}

""")
        
        if is_image:
            prompt_builder.write("è¯·åˆ†æå›¾ç‰‡OCRè¯†åˆ«çš„å†…å®¹ï¼Œå¹¶ç»“åˆç›¸å…³æ–‡æ¡£æä¾›å‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸å›¾ç‰‡å†…å®¹æ— å…³ï¼Œè¯·ä¼˜å…ˆåŸºäºå›¾ç‰‡å†…å®¹å›ç­”ã€‚")
        else:
            prompt_builder.write("è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä¸å­˜åœ¨äºæ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚")
        
        # è·å–æœ€ç»ˆæç¤ºæ–‡æœ¬
        prompt = prompt_builder.getvalue()
        
        # ç¼“å­˜ç”Ÿæˆçš„æç¤º
        self.retrieval_cache[cache_key] = {
            'results': prompt,
            'timestamp': time.time()
        }
        
        return prompt
    
    def get_last_retrieved_documents(self) -> List[Dict]:
        """
        è·å–ä¸Šæ¬¡æ£€ç´¢çš„æ–‡æ¡£ (å…¼å®¹æ—§ç‰ˆAPI)
        """
        if hasattr(self, 'last_retrieval') and self.last_retrieval:
            return self.last_retrieval.get('results', [])
        # æ—§ç‰ˆæœ¬å…¼å®¹æ€§
        if hasattr(self, 'last_query') and self.last_query:
            cache_key = self.last_query.get('cache_key')
            if cache_key in self.retrieval_cache:
                return self.retrieval_cache[cache_key]['results']
        return []