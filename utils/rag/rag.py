from utils.document_loader import CSVLoader, MDLoader, PDFLoader, TXTLoader, DocxLoader, HTMLLoader
from utils.load_config import configs
from utils.base_func import *
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from tqdm import tqdm
from FlagEmbedding import FlagModel, FlagReranker
import numpy as np
import json
import os
import time

class Rag:
    def __init__(self):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        - æ”¯æŒå¤šç§æ–‡æ¡£åŠ è½½å™¨
        - è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„æ–‡æ¡£å’Œå‘é‡
        - æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£
        - æ”¯æŒå¤šè½®å¯¹è¯
        """
        self.config = configs
        self.documents_path = Path(self.config['rag']['document_path'])
        self.files = self.get_all_files_in_directory()
        print('documents files:', self.files)
        self.vector_store_path = Path(self.config['rag']['vector_store']['index_path'])
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¶è‡ªåŠ¨åŠ è½½å·²æœ‰æ•°æ®
        self.docs = self._load_metadata()
        self.doc_vectors = self._load_vectors()

        self.device = self.config['rag']['embedding_model']['device']
        self.embedding_model = FlagModel(self.config['rag']['embedding_model']['path'], 
                  query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                  use_fp16=True,devices=self.device)
        self.embedding_model.dimension = self.config['rag']['embedding_model']['dimension']
        
        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        self.reranker_device = self.config['rag']['reranker_model']['device']
        self.reranker_model = FlagReranker(
            self.config['rag']['reranker_model']['path'],
            use_fp16=True,
            device=self.reranker_device
        )
        
        # æ£€ç´¢å‚æ•°
        self.top_k = self.config['rag']['retrieval']['top_k']
        self.initial_retrieval_k = self.config['rag']['retrieval']['initial_retrieval_k']
        self.score_threshold = self.config['rag']['retrieval']['score_threshold']
        self.rerank_threshold = self.config['rag']['retrieval']['rerank_score_threshold']
        self.similarity_metric = self.config['rag']['vector_store']['similarity_metric']
        
        # å¤šè½®å¯¹è¯å†å²
        self.conversation_history = []
        self.max_history_length = 5  # ä¿ç•™5æ¡å†å²è®°å½•
    
    def add_to_history(self, query: str, response: str) -> None:
        """
        å°†å½“å‰çš„å¯¹è¯æ·»åŠ åˆ°å†å²è®°å½•ä¸­
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            response: ç³»ç»Ÿå›å¤
        """
        self.conversation_history.append({"user": query, "assistant": response})
        
        # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡è®¾å®šçš„æœ€å¤§é•¿åº¦
        if len(self.conversation_history) > self.max_history_length:
            self.conversation_history = self.conversation_history[-self.max_history_length:]
    
    def get_conversation_context(self) -> str:
        """
        è·å–æ ¼å¼åŒ–çš„å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        
        è¿”å›:
            æ ¼å¼åŒ–çš„å¯¹è¯å†å²å­—ç¬¦ä¸²
        """
        if not self.conversation_history:
            return ""
            
        context = "ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯å†å²ï¼š\n\n"
        for i, exchange in enumerate(self.conversation_history):
            context += f"ç”¨æˆ·: {exchange['user']}\n"
            context += f"åŠ©æ‰‹: {exchange['assistant']}\n\n"
        
        return context
    
    def clear_history(self) -> None:
        """
        æ¸…ç©ºå¯¹è¯å†å²
        """
        self.conversation_history = []
    
    def _generate_chunk_summary(self, chunk_content: str, max_length: int = 150) -> str:
        """
        ä½¿ç”¨è¯­è¨€æ¨¡å‹ä¸ºæ–‡æ¡£å—ç”Ÿæˆæ ‡é¢˜/ç®€ä»‹
        
        å‚æ•°:
            chunk_content: æ–‡æ¡£å—å†…å®¹
            max_length: æ ‡é¢˜æœ€å¤§é•¿åº¦
            
        è¿”å›:
            ç”Ÿæˆçš„æ ‡é¢˜/ç®€ä»‹
        """
        try:

            # å‡†å¤‡æç¤º
            prompt = f"""           
            ç›´æ¥å¼€å§‹ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œä»…åˆ—æ ¸å¿ƒè¦ç‚¹ï¼šé¦–å…ˆæ˜¯æ€»ç»“å‡ºæ¥çš„æ ‡é¢˜ï¼Œå†ç”¨ç¬¦å·ã€Œâ€¢ã€åˆ†é¡¹ï¼Œæœ€åç”¨ç¬¦å·ã€Œâ†’ã€æ€»ç»“ã€‚é¿å…ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚(ä¸è¶…è¿‡{max_length}ä¸ªå­—ç¬¦ï¼‰
          
            æ–‡æœ¬å†…å®¹:
            {chunk_content}
            """
            response = call_language_model(prompt=prompt)
            response = remove_think_tag(response)
            summary = response.strip()
            # å¦‚æœæ ‡é¢˜è¿‡é•¿ï¼Œæˆªæ–­
            if len(summary) > max_length:
                summary = summary[:max_length-3] + "..."
            return summary
                
        except Exception as e:
            print(f"ç”Ÿæˆæ ‡é¢˜æ—¶å‡ºé”™: {str(e)}")
            return self._generate_default_summary(chunk_content)
    
    def _generate_default_summary(self, chunk_content: str, max_length: int = 150) -> str:
        """
        å½“LLMç”Ÿæˆå¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€å•è§„åˆ™ç”Ÿæˆé»˜è®¤æ ‡é¢˜
        """
        # ç®€å•å–å†…å®¹çš„å‰éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
        text = chunk_content.strip().replace('\n', ' ')
        if len(text) > max_length:
            summary = text[:max_length-3] + "..."
        else:
            summary = text
        return summary
    
    def get_all_files_in_directory(self) -> List[str]:
        directory_path = Path(self.documents_path)  # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ª Path å¯¹è±¡
        return [str(file) for file in directory_path.rglob('*') if file.is_file()]

    def _get_vector_path(self) -> Path:
        """è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "doc_vectors.npy"

    def _get_metadata_path(self) -> Path:
        """è·å–å…ƒæ•°æ®å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "metadata.json"

    def _load_vectors(self) -> np.ndarray:
        """åŠ è½½å·²ä¿å­˜çš„å‘é‡"""
        vector_path = self._get_vector_path()
        if vector_path.exists():
            return np.load(vector_path)
        return None

    def _load_metadata(self) -> List[Dict]:
        """åŠ è½½å·²ä¿å­˜çš„å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path()
        if metadata_path.exists():
            with open(metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def _save_data(self,vectors=True,docs=True):
        """ä¿å­˜å½“å‰æ•°æ®åˆ°ç£ç›˜"""
        # ä¿å­˜å‘é‡
        if vectors and self.doc_vectors is not None:
            np.save(self._get_vector_path(), self.doc_vectors)
        
        # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰æ–°çš„å…ƒæ•°æ®æ ¼å¼å­—æ®µ
        if docs:
            for doc in self.docs:
                if 'chunk_index' not in doc:
                    doc['chunk_index'] = 0
                if 'chunk_content' not in doc:
                    doc['chunk_content'] = doc.get('content', '')
                if 'total_chunks' not in doc:
                    # è®¡ç®—æ­¤æ–‡ä»¶çš„æ€»å—æ•°
                    file_path = doc.get('file_path')
                    same_file_docs = [d for d in self.docs if d.get('file_path') == file_path]
                    doc['total_chunks'] = len(same_file_docs)
                if 'chunk_summary' not in doc:
                    # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆä¸€ä¸ªé»˜è®¤æ ‡é¢˜
                    doc['chunk_summary'] = self._generate_default_summary(doc['chunk_content'])
            # ä¿å­˜å…ƒæ•°æ®
            with open(self._get_metadata_path(), "w", encoding="utf-8") as f:
                json.dump(self.docs, f, ensure_ascii=False, indent=2)
    
    def _cosine_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - å‘é‡åŒ–è®¡ç®—
        
        ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        cos(Î¸) = (AÂ·B) / (|A|Â·|B|)
        å…¶ä¸­,Aå’ŒBæ˜¯ä¸¤ä¸ªå‘é‡,AÂ·Bæ˜¯å®ƒä»¬çš„ç‚¹ç§¯,|A|å’Œ|B|æ˜¯å®ƒä»¬çš„æ¨¡ã€‚
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # è®¡ç®—ç‚¹ç§¯
        dot_products = np.dot(query_vector, doc_vectors.T).flatten()
        
        # è®¡ç®—æ¨¡
        query_norm = np.linalg.norm(query_vector)
        doc_norms = np.linalg.norm(doc_vectors, axis=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = dot_products / (query_norm * doc_norms)
        
        return similarities
    
    def _l2_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—L2ç›¸ä¼¼åº¦ï¼ˆæ¬§æ°è·ç¦»ï¼‰- å‘é‡åŒ–è®¡ç®—
        
        L2ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        d(A,B) = sqrt(sum((A_i - B_i)^2))
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        """
        # è®¡ç®—æ¬§æ°è·ç¦»
        # ä½¿ç”¨å¹¿æ’­æœºåˆ¶ï¼š(1, dim) - (num_docs, dim) => (num_docs, dim)
        distances = np.linalg.norm(query_vector - doc_vectors, axis=1)
        
        # ç”±äºæ¬§æ°è·ç¦»è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œä¸ºäº†ä¿æŒä¸ä½™å¼¦ç›¸ä¼¼åº¦ä¸€è‡´ï¼ˆå€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
        # æˆ‘ä»¬å¯ä»¥å¯¹è·ç¦»å–è´Ÿå€¼æˆ–å€’æ•°ï¼Œè¿™é‡Œé€‰æ‹©å–è´Ÿå€¼
        similarities = -distances
        
        return similarities
        
    def load_documents(self, file_paths: List[str]) -> None:
        """
        åŠ è½½å¤šç§æ ¼å¼çš„æ–‡æ¡£
        """
        # è½¬æ¢æ‰€æœ‰è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
        abs_paths = [str(Path(fp).absolute()) for fp in file_paths]
        
        # è¿‡æ»¤å·²åŠ è½½æ–‡ä»¶ï¼ˆä½¿ç”¨é›†åˆæé«˜æŸ¥è¯¢æ•ˆç‡ï¼‰
        existing_files = {doc["file_path"] for doc in self.docs}
        new_files = [fp for fp in abs_paths if fp not in existing_files]
        
        if not new_files:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²åŠ è½½è¿‡")
            return

        try:
            # å°†numpyæ•°ç»„è½¬ä¸ºåˆ—è¡¨ä¾¿äºè¿½åŠ 
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = []

            with tqdm(new_files, desc="ğŸ“ æ€»ä½“è¿›åº¦") as global_pbar:
                for file_path in global_pbar:
                    global_pbar.set_postfix_str(f'å¤„ç†ä¸­: {Path(file_path).name}')
                    
                    try:
                        # è·å–æ–‡ä»¶åŠ è½½å™¨
                        if file_path.endswith('.csv'):
                            loader = CSVLoader()
                        elif file_path.endswith('.docx'):
                            loader = DocxLoader()
                        elif file_path.endswith('.md'):
                            loader = MDLoader()
                        elif file_path.endswith('.pdf'):
                            loader = PDFLoader()
                        elif file_path.endswith('.html') or file_path.endswith('.htm'):
                            loader = HTMLLoader()
                        elif (file_path.endswith('.txt') or (Path(file_path).suffix == '' and Path(file_path).is_file())):
                            loader = TXTLoader()
                        else:
                            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

                        # åŠ è½½æ–‡æ¡£å†…å®¹
                        chunks = loader.load(file_path)
                        total_chunks = len(chunks)
                        
                        # å¤„ç†æ–‡æ¡£å—
                        file_vectors = []
                        for i, chunk in enumerate(tqdm(chunks, desc=f"ğŸ“„ {Path(file_path).name}", leave=True)):
                            chunk_content = str(chunk)
                            
                            # ä½¿ç”¨æ¨¡å‹ä¸ºæ–‡æ¡£å—ç”Ÿæˆæ ‡é¢˜
                            chunk_summary = self._generate_chunk_summary(chunk_content)
                            
                            # å­˜å‚¨å…ƒæ•°æ® - ä½¿ç”¨æ–°çš„æ ¼å¼
                            self.docs.append({
                                "file_path": file_path,
                                "chunk_index": i,  # è®°å½•å—ç´¢å¼•
                                "chunk_content": chunk_content,  # æ–‡æ¡£å†…å®¹
                                "chunk_summary": chunk_summary,  # æ–°å¢ï¼šæ–‡æ¡£å—æ ‡é¢˜
                                "total_chunks": total_chunks,  # è®°å½•æ€»å—æ•°
                                "timestamp": time.time()  # æ·»åŠ æ—¶é—´æˆ³ç”¨äºç‰ˆæœ¬æ§åˆ¶
                            })
                            
                            # ç»“åˆæ‘˜è¦å’Œå†…å®¹ç”Ÿæˆå‘é‡ï¼Œå¢å¼ºè¯­ä¹‰ç†è§£
                            indexed_text = f"{chunk_summary}\n{chunk_summary}\n{chunk_content}"
                            file_vectors.append(self.embedding_model.encode(indexed_text))
                            
                        # è¿½åŠ å‘é‡
                        vectors.extend(file_vectors)

                    except Exception as e:
                        print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                        continue

            # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„
            self.doc_vectors = np.array(vectors)
            
        finally:
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_data()

    def reset(self):
        """é‡ç½®æ‰€æœ‰å­˜å‚¨æ•°æ®"""
        self.doc_vectors = None
        self.docs = []
        if self._get_vector_path().exists():
            os.remove(self._get_vector_path())
        if self._get_metadata_path().exists():
            os.remove(self._get_metadata_path())
    
    def rebuild_vector_db(self, file_path=None, chunk_indices=None):
        """
        é‡å»ºå‘é‡æ•°æ®åº“
        åœ¨ä¿®æ”¹äº†å…ƒæ•°æ®ä¸­çš„æ–‡æ¡£å†…å®¹åè°ƒç”¨æ­¤æ–¹æ³•æ¥æ›´æ–°å‘é‡
        
        å‚æ•°:
            file_path: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡å»ºçš„æ–‡ä»¶è·¯å¾„
            chunk_indices: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡å»ºçš„chunkç´¢å¼•åˆ—è¡¨(å½“file_pathæä¾›æ—¶æœ‰æ•ˆ)
        """
        if not self.docs:
            print("æ— å¯ç”¨æ–‡æ¡£ï¼Œæ— æ³•é‡å»ºå‘é‡åº“")
            return
        
        try:
            # å¦‚æœå·²æœ‰å‘é‡å­˜å‚¨ï¼ŒåŠ è½½å®ƒ
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = [None] * len(self.docs)
            
            # ç¡®å®šéœ€è¦é‡å»ºçš„æ–‡æ¡£ç´¢å¼•
            rebuild_indices = []
            if file_path is not None:
                # åªé‡å»ºæŒ‡å®šæ–‡ä»¶çš„æŒ‡å®šå—
                file_path = os.path.abspath(file_path)
                for i, doc in enumerate(self.docs):
                    if doc.get('file_path') == file_path:
                        if chunk_indices is None or int(doc.get('chunk_index', 0)) in chunk_indices:
                            rebuild_indices.append(i)
                print(f"å°†é‡å»ºæ–‡ä»¶ {file_path} çš„ {len(rebuild_indices)} ä¸ªåˆ†å—å‘é‡")
            else:
                # é‡å»ºæ‰€æœ‰æ–‡æ¡£
                rebuild_indices = list(range(len(self.docs)))
                print(f"å°†é‡å»ºæ‰€æœ‰ {len(rebuild_indices)} ä¸ªåˆ†å—å‘é‡")
            
            if not rebuild_indices:
                print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡å»ºçš„æ–‡æ¡£")
                return
                
            print("æ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“...")
            
            with tqdm(rebuild_indices, desc="ğŸ“ é‡å»ºå‘é‡è¿›åº¦") as pbar:
                for i in pbar:
                    doc = self.docs[i]
                    chunk_content = doc.get('chunk_content', '')
                    chunk_summary = doc.get('chunk_summary', '')
                    
                    if chunk_content:
                        # ç»„åˆå†…å®¹å’Œæ‘˜è¦ï¼Œç»™äºˆæ‘˜è¦æ›´é«˜çš„æƒé‡
                        if chunk_summary:
                            # é‡å¤æ‘˜è¦å†…å®¹ä»¥å¢åŠ å…¶åœ¨å‘é‡ä¸­çš„æƒé‡
                            indexed_text = f"{chunk_summary}\n{chunk_summary}\n{chunk_content}"
                        else:
                            indexed_text = chunk_content
                            
                        # ç”Ÿæˆæ–°çš„å‘é‡
                        vector = self.embedding_model.encode(indexed_text)
                        vectors[i] = vector
                    else:
                        print(f"è­¦å‘Š: æ–‡æ¡£ {doc.get('file_path')} æ²¡æœ‰å†…å®¹ï¼Œè·³è¿‡")
                        # æ·»åŠ ä¸€ä¸ªç©ºå‘é‡ä»¥ä¿æŒç´¢å¼•å¯¹é½
                        vectors[i] = np.zeros(self.embedding_model.dimension)
            
            # æ›´æ–°å‘é‡å­˜å‚¨
            self.doc_vectors = np.array(vectors)
            
            # ä¿å­˜åˆ°ç£ç›˜
            self._save_data()
            
            print(f"å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼Œæ›´æ–°äº† {len(rebuild_indices)} ä¸ªæ–‡æ¡£å‘é‡")
        
        except Exception as e:
            print(f"é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            raise
    
    def _enhance_query(self, query: str) -> str:
        """
        ä½¿ç”¨ä¸“é—¨çš„æç¤ºæ¥å¢å¼ºåŸå§‹æŸ¥è¯¢ï¼Œæå–å…³é”®æ¦‚å¿µå¹¶æ‰©å±•æŸ¥è¯¢
        
        å‚æ•°:
            query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            
        è¿”å›:
            å¢å¼ºåçš„æŸ¥è¯¢
        """
        try:
            # å¯¹äºçŸ­æŸ¥è¯¢ï¼Œå¯èƒ½ä¸éœ€è¦å¢å¼º
            if len(query) < 30 and not self.conversation_history:
                return query
                
            # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡
            conversation_context = self.get_conversation_context()
            
            # æŸ¥è¯¢å¢å¼ºä¸“ç”¨æç¤º
            prompt = f"""
            {conversation_context if conversation_context else ""}
            åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œæå–æ ¸å¿ƒæ¦‚å¿µã€å…³é”®è¯å’Œå®ä½“ã€‚ç„¶åï¼Œé‡å†™æ­¤æŸ¥è¯¢ä»¥ä¾¿æ›´å¥½åœ°ç”¨äºæ–‡æ¡£æ£€ç´¢ã€‚
            ä½ çš„è¾“å‡ºåº”è¯¥æ˜¯åŸå§‹æŸ¥è¯¢çš„æ‰©å±•ç‰ˆæœ¬ï¼ŒåŒ…å«åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µã€‚
            ä¸è¦ä½¿ç”¨markdownï¼Œä¸è¦åˆ†ç‚¹ï¼Œç›´æ¥è¾“å‡ºå¢å¼ºåçš„æŸ¥è¯¢æ–‡æœ¬ã€‚
            ä¸è¦ä½¿ç”¨ä»»ä½•å‰ç¼€æˆ–è§£é‡Šï¼Œåªè¿”å›å¢å¼ºåçš„æŸ¥è¯¢ã€‚
            
            ç”¨æˆ·åŸå§‹æŸ¥è¯¢: {query}
            å¢å¼ºåçš„æŸ¥è¯¢:
            """
            
            enhanced_query = call_language_model(prompt=prompt)
            enhanced_query = remove_think_tag(enhanced_query).strip()
            
            # å¦‚æœå¢å¼ºå¤±è´¥æˆ–ç»“æœå¼‚å¸¸ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢
            if not enhanced_query or len(enhanced_query) < len(query)/2:
                return query
                
            # ç»„åˆåŸå§‹æŸ¥è¯¢å’Œå¢å¼ºæŸ¥è¯¢ï¼Œä¿ç•™åŸå§‹å«ä¹‰çš„åŒæ—¶æ‰©å±•æœç´¢èŒƒå›´
            final_query = f"{enhanced_query}\n{query}"
            return final_query
                
        except Exception as e:
            print(f"æŸ¥è¯¢å¢å¼ºå¤±è´¥: {str(e)}")
            return query  # å‡ºé”™æ—¶è¿”å›åŸå§‹æŸ¥è¯¢
            
    def retrieve_documents(self, query: str, top_k: int = None, threshold: float = None, rerank_threshold: float = None, use_history: bool = True) -> List[Dict]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå…ˆä½¿ç”¨embeddingæ¨¡å‹æ£€ç´¢ï¼Œå†ä½¿ç”¨rerankeræ¨¡å‹é‡æ’åº
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®å€¼
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
            rerank_threshold: é‡æ’åºåˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
            use_history: æ˜¯å¦åœ¨æ£€ç´¢æ—¶è€ƒè™‘å¯¹è¯å†å²
            
        è¿”å›:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        """
        if top_k is None:
            top_k = self.top_k
            
        if threshold is None:
            threshold = self.score_threshold
        
        if rerank_threshold is None:
            rerank_threshold = self.rerank_threshold
        
        if not self.docs or self.doc_vectors is None:
            print("æ— å¯ç”¨æ–‡æ¡£")
            return []

        # ä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•å¢å¼ºæŸ¥è¯¢ï¼Œå¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨å†å²
        try:
            enhanced_query = self._enhance_query(query)
        except:
            enhanced_query = query
            print("æŸ¥è¯¢å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode(enhanced_query).reshape(1, -1)  # ç¡®ä¿å½¢çŠ¶æ˜¯ (1, dim)
        
        # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
        if self.similarity_metric == 'cosine':
            similarities = self._cosine_similarity(query_vector, self.doc_vectors)
        elif self.similarity_metric == 'l2':
            similarities = self._l2_similarity(query_vector, self.doc_vectors)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡ï¼š{self.similarity_metric}")

        # æ‰¾åˆ°ç›¸ä¼¼åº¦å¾—åˆ†æœ€é«˜çš„initial_retrieval_kä¸ªæ–‡æ¡£
        initial_k = self.initial_retrieval_k
        indices = np.argsort(similarities)[::-1][:initial_k]
        # æ”¶é›†åˆæ­¥æ£€ç´¢ç»“æœ
        initial_results = []
        for idx in indices:
            if similarities[idx] >= threshold:  # ä»ç„¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                doc = dict(self.docs[idx])  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
                doc['initial_score'] = float(similarities[idx])  # ä¿å­˜åˆå§‹å¾—åˆ†
                
                # ç¡®ä¿æ–‡æ¡£å…·æœ‰æ‰€éœ€çš„å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                if 'chunk_index' not in doc:
                    doc['chunk_index'] = 0
                if 'chunk_content' not in doc:
                    doc['chunk_content'] = doc.get('chunk_content', '')
                if 'total_chunks' not in doc:
                    # è®¡ç®—æ­¤æ–‡ä»¶çš„æ€»å—æ•°
                    same_file_docs = [d for d in self.docs if d.get('file_path') == doc.get('file_path')]
                    doc['total_chunks'] = len(same_file_docs)
                if 'chunk_summary' not in doc:
                    # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤æ ‡é¢˜
                    doc['chunk_summary'] = self._generate_default_summary(doc['chunk_content'])
                    
                initial_results.append(doc)
        
        if not initial_results:
            print("åˆæ­¥æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return []
            
        # ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨rerankeræ¨¡å‹è¿›è¡Œé‡æ’åº
        pairs = []
        
        # é‡æ’åºæ—¶è€ƒè™‘å¯¹è¯å†å²
        rerank_query = query
        if use_history and self.conversation_history:
            context_str = ""
            # åªä½¿ç”¨æœ€è¿‘çš„å‡ è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            for exchange in self.conversation_history[-3:]:  # ä½¿ç”¨æœ€è¿‘3è½®å¯¹è¯
                context_str += f"ç”¨æˆ·: {exchange['user']}\n"
                context_str += f"åŠ©æ‰‹: {exchange['assistant']}\n"
            rerank_query = f"{context_str}\nå½“å‰é—®é¢˜: {query}"
        
        for doc in initial_results:
            # åœ¨é‡æ’åºæ—¶ï¼Œè€ƒè™‘æ‘˜è¦ä¿¡æ¯
            context = f"{doc['chunk_summary']}\n\n{doc['chunk_content']}"
            pairs.append((rerank_query, context))
        
        # è°ƒç”¨rerankeræ¨¡å‹è·å–é‡æ’åºåˆ†æ•°
        rerank_scores = self.reranker_model.compute_score(pairs)
        
        # å°†rerankeråˆ†æ•°å½’ä¸€åŒ–åˆ°0-1èŒƒå›´å†…ï¼ˆä½¿ç”¨sigmoidå‡½æ•°ï¼‰
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        
        # ä¸ºæ–‡æ¡£æ·»åŠ é‡æ’åºåˆ†æ•°ï¼ˆå½’ä¸€åŒ–åçš„ï¼‰
        reranked_results = []
        for i, doc in enumerate(initial_results):
            # å¤åˆ¶æ–‡æ¡£ä»¥ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µ
            reranked_doc = dict(doc)
            # ä½¿ç”¨sigmoidå°†å¾—åˆ†å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            normalized_score = float(sigmoid(rerank_scores[i]))
            reranked_doc['score'] = normalized_score  # rerankerå½’ä¸€åŒ–åˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
            if reranked_doc['score'] >= rerank_threshold:
                reranked_results.append(reranked_doc)
        
        # æŒ‰é‡æ’åºåˆ†æ•°é™åºæ’åº
        reranked_results = sorted(reranked_results, key=lambda x: x['score'], reverse=True)
        
        # è¿”å›top_kä¸ªæ–‡æ¡£
        final_results = reranked_results[:top_k]
        
        return final_results
    
    def generate_prompt(self, query: str, top_k: int = None, threshold: float = None, rerank_threshold: float = None, is_image: bool = False, use_history: bool = True) -> str:
        """
        ç”Ÿæˆ RAG æç¤º
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param top_k: è¿”å›æœ€ç›¸å…³çš„ top_k æ–‡æ¡£
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :param rerank_threshold: é‡æ’åºåˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :param is_image: æ˜¯å¦ä¸ºå›¾ç‰‡æŸ¥è¯¢
        :param use_history: æ˜¯å¦åœ¨æç¤ºä¸­åŒ…å«å¯¹è¯å†å²
        :return: ç”Ÿæˆçš„æç¤ºæ–‡æœ¬
        """
        if top_k is None:
            top_k = self.top_k  
        if threshold is None:
            threshold = self.score_threshold
        if rerank_threshold is None:
            rerank_threshold = self.rerank_threshold
            
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.retrieve_documents(query, top_k, threshold, rerank_threshold, use_history)
        
        # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        conversation_context = self.get_conversation_context() if use_history and self.conversation_history else ""
        
        if is_image:
            # å›¾ç‰‡æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿
            prompt = f"""
            {conversation_context}
            è¯·æ ¹æ®OCRè¯†åˆ«ç»“æœå’Œç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·å…³äºå›¾ç‰‡çš„é—®é¢˜ã€‚

            å›¾ç‰‡å†…å®¹å’Œç”¨æˆ·æé—®: {query}

            ç›¸å…³æ–‡æ¡£:\n
            """
        else:
            # å¸¸è§„æ–‡æœ¬æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿
            prompt = f"""
            {conversation_context}
            è¯·æ ¹æ®ç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜ã€‚è‹¥æœ‰çš„æ–‡æ¡£ä¸ç›¸å…³ï¼Œå°½é‡ä¸è¦è¾“å‡ºä¸ä¸ç›¸å…³æ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ ¹æ®ä½ è‡ªå·±æ¥è¾“å‡ºã€‚
            
            {"å½“å‰" if conversation_context else ""}ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜: {query}

            ç›¸å…³æ–‡æ¡£:\n
            """
            
        for i, doc in enumerate(relevant_docs):
            # æ·»åŠ æ–‡æ¡£æ ‡é¢˜å’Œå†…å®¹
            prompt += f"""
            æ–‡æ¡£ {i+1} [æ¥è‡ª: {doc['file_path']}]:
            æ€»ç»“: {doc.get('chunk_summary', 'æ— æ€»ç»“')}
            
            å†…å®¹:
            {doc['chunk_content']}
            ç›¸ä¼¼åº¦å¾—åˆ†: {doc['score']:.4f}\n\n
            """
        
        if is_image:
            prompt += "è¯·åˆ†æå›¾ç‰‡OCRè¯†åˆ«çš„å†…å®¹ï¼Œå¹¶ç»“åˆç›¸å…³æ–‡æ¡£æä¾›å‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸å›¾ç‰‡å†…å®¹æ— å…³ï¼Œè¯·ä¼˜å…ˆåŸºäºå›¾ç‰‡å†…å®¹å›ç­”ã€‚"
        else:
            if conversation_context:
                prompt += "è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹å¹¶è€ƒè™‘å¯¹è¯å†å²ï¼Œè¿è´¯åœ°å›ç­”ç”¨æˆ·å½“å‰çš„é—®é¢˜ã€‚å›ç­”åº”å½“ä¸ä¹‹å‰çš„å¯¹è¯ä¿æŒä¸€è‡´æ€§ã€‚"
            else:
                prompt += "è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä¸å­˜åœ¨äºæ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚"
            
        return prompt