from utils.document_loader import CSVLoader, MDLoader, PDFLoader, TXTLoader, DocxLoader
from utils.load_config import configs
from utils.base_func import parse_response
from typing import List, Dict
from pathlib import Path
import numpy as np
from tqdm import tqdm
from FlagEmbedding import FlagModel
import numpy as np
import json
import os
import time

class Rag:
    def __init__(self):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        - æ”¯æŒå¤šç§æ–‡æ¡£åŠ è½½å™¨
        - è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„æ–‡æ¡£å’Œå‘é‡
        - æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£
        """
        self.config = configs
        self.documents_path = Path(self.config['rag']['document_path'])
        self.files = self.get_all_files_in_directory()
        print('documents files:', self.files)
        self.vector_store_path = Path(self.config['rag']['vector_store']['index_path'])
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¶è‡ªåŠ¨åŠ è½½å·²æœ‰æ•°æ®
        self.docs = self._load_metadata()
        self.doc_vectors = self._load_vectors()

        self.device = self.config['rag']['embedding_model']['device']
        self.embedding_model = FlagModel(self.config['rag']['embedding_model']['path'], 
                  query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                  use_fp16=True,devices=self.device)

        self.top_k = self.config['rag']['retrieval']['top_k']
        self.score_threshold = self.config['rag']['retrieval']['score_threshold']
        self.similarity_metric = self.config['rag']['vector_store']['similarity_metric']

        # self.reranker_model = 
    
    def get_all_files_in_directory(self) -> List[str]:
        directory_path = Path(self.documents_path)  # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ª Path å¯¹è±¡
        return [str(file) for file in directory_path.rglob('*') if file.is_file()]

    def _get_vector_path(self) -> Path:
        """è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "doc_vectors.npy"

    def _get_metadata_path(self) -> Path:
        """è·å–å…ƒæ•°æ®å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "metadata.json"

    def _load_vectors(self) -> np.ndarray:
        """åŠ è½½å·²ä¿å­˜çš„å‘é‡"""
        vector_path = self._get_vector_path()
        if vector_path.exists():
            return np.load(vector_path)
        return None

    def _load_metadata(self) -> List[Dict]:
        """åŠ è½½å·²ä¿å­˜çš„å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path()
        if metadata_path.exists():
            with open(metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def _save_data(self):
        """ä¿å­˜å½“å‰æ•°æ®åˆ°ç£ç›˜"""
        # ä¿å­˜å‘é‡
        if self.doc_vectors is not None:
            np.save(self._get_vector_path(), self.doc_vectors)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(self._get_metadata_path(), "w", encoding="utf-8") as f:
            json.dump(self.docs, f, ensure_ascii=False, indent=2)
    
    def _cosine_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - å‘é‡åŒ–è®¡ç®—
        
        ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        cos(Î¸) = (AÂ·B) / (|A|Â·|B|)
        å…¶ä¸­,Aå’ŒBæ˜¯ä¸¤ä¸ªå‘é‡,AÂ·Bæ˜¯å®ƒä»¬çš„ç‚¹ç§¯,|A|å’Œ|B|æ˜¯å®ƒä»¬çš„æ¨¡ã€‚
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # è®¡ç®—ç‚¹ç§¯
        dot_products = np.dot(query_vector, doc_vectors.T).flatten()
        
        # è®¡ç®—æ¨¡
        query_norm = np.linalg.norm(query_vector)
        doc_norms = np.linalg.norm(doc_vectors, axis=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = dot_products / (query_norm * doc_norms)
        
        return similarities
    
    def _l2_similarity(self, query_vector, doc_vectors):
        """
        è®¡ç®—L2ç›¸ä¼¼åº¦ï¼ˆæ¬§æ°è·ç¦»ï¼‰- å‘é‡åŒ–è®¡ç®—
        
        L2ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        d(A,B) = sqrt(sum((A_i - B_i)^2))
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
            doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        è¿”å›:
            ä¸€ç»´numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        """
        # è®¡ç®—æ¬§æ°è·ç¦»
        # ä½¿ç”¨å¹¿æ’­æœºåˆ¶ï¼š(1, dim) - (num_docs, dim) => (num_docs, dim)
        distances = np.linalg.norm(query_vector - doc_vectors, axis=1)
        
        # ç”±äºæ¬§æ°è·ç¦»è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œä¸ºäº†ä¿æŒä¸ä½™å¼¦ç›¸ä¼¼åº¦ä¸€è‡´ï¼ˆå€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
        # æˆ‘ä»¬å¯ä»¥å¯¹è·ç¦»å–è´Ÿå€¼æˆ–å€’æ•°ï¼Œè¿™é‡Œé€‰æ‹©å–è´Ÿå€¼
        similarities = -distances
        
        return similarities
        
    def load_documents(self, file_paths: List[str]) -> None:
        """
        åŠ è½½å¤šç§æ ¼å¼çš„æ–‡æ¡£
        """
        # è½¬æ¢æ‰€æœ‰è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
        abs_paths = [str(Path(fp).absolute()) for fp in file_paths]
        
        # è¿‡æ»¤å·²åŠ è½½æ–‡ä»¶ï¼ˆä½¿ç”¨é›†åˆæé«˜æŸ¥è¯¢æ•ˆç‡ï¼‰
        existing_files = {doc["file_path"] for doc in self.docs}
        new_files = [fp for fp in abs_paths if fp not in existing_files]
        
        if not new_files:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²åŠ è½½è¿‡")
            return

        try:
            # å°†numpyæ•°ç»„è½¬ä¸ºåˆ—è¡¨ä¾¿äºè¿½åŠ 
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = []

            with tqdm(new_files, desc="ğŸ“ æ€»ä½“è¿›åº¦") as global_pbar:
                for file_path in global_pbar:
                    global_pbar.set_postfix_str(f'å¤„ç†ä¸­: {Path(file_path).name}')
                    
                    try:
                        # è·å–æ–‡ä»¶åŠ è½½å™¨
                        if file_path.endswith('.csv'):
                            loader = CSVLoader()
                        elif file_path.endswith('.docx'):
                            loader = DocxLoader()
                        elif file_path.endswith('.md'):
                            loader = MDLoader()
                        elif file_path.endswith('.pdf'):
                            loader = PDFLoader()
                        elif (file_path.endswith('.txt') or (Path(file_path).suffix == '' and Path(file_path).is_file())):
                            loader = TXTLoader()
                        else:
                            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

                        # åŠ è½½æ–‡æ¡£å†…å®¹
                        chunks = loader.load(file_path)
                        
                        # å¤„ç†æ–‡æ¡£å—
                        file_vectors = []
                        for chunk in tqdm(chunks, desc=f"ğŸ“„ {Path(file_path).name}", leave=True):
                            # å­˜å‚¨å…ƒæ•°æ®
                            self.docs.append({
                                "file_path": file_path,
                                "content": str(chunk),
                                "timestamp": time.time()  # æ·»åŠ æ—¶é—´æˆ³ç”¨äºç‰ˆæœ¬æ§åˆ¶
                            })
                            # ç”Ÿæˆå‘é‡
                            file_vectors.append(self.embedding_model.encode(str(chunk)))
                        # è¿½åŠ å‘é‡
                        vectors.extend(file_vectors)

                    except Exception as e:
                        print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                        continue

            # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„
            self.doc_vectors = np.array(vectors)
            
        finally:
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_data()

    def reset(self):
        """é‡ç½®æ‰€æœ‰å­˜å‚¨æ•°æ®"""
        self.doc_vectors = None
        self.docs = []
        if self._get_vector_path().exists():
            os.remove(self._get_vector_path())
        if self._get_metadata_path().exists():
            os.remove(self._get_metadata_path())
    
    def retrieve_documents(self, query: str, top_k: int = None, threshold: float = 0.4) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶è¿”å›å¸¦è·¯å¾„çš„ç»“æœ
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: è¿”å›ç»“æœæ•°é‡
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :return: åŒ…å«è·¯å¾„å’Œå†…å®¹çš„æ–‡æ¡£åˆ—è¡¨
        """
        if top_k is None:
            top_k = self.top_k  # ä½¿ç”¨ç±»å±æ€§ top_k ä½œä¸ºé»˜è®¤å€¼
        if self.doc_vectors is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ–‡æ¡£")

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        # query_vector = self.embedding_model.encode(query).reshape(1, -1)
        query_vector = self.embedding_model.encode_queries([query])
        # print(query_vector)
        # query_vector.shape: (1, 1024)
        # doc_vectors.shape: (num_docs, 1024)
        # è®¡ç®—ç›¸ä¼¼åº¦
        if self.similarity_metric == "cosine":
            similarities = self._cosine_similarity(query_vector, self.doc_vectors)
        elif self.similarity_metric == "l2":
            similarities = self._l2_similarity(query_vector, self.doc_vectors)
            # å¯¹äºL2è·ç¦»ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯è´Ÿè·ç¦»ï¼Œéœ€è¦è°ƒæ•´é˜ˆå€¼
            threshold = -threshold if threshold > 0 else threshold
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼: {self.similarity_metric}")
        
        # ç¡®ä¿ top_k ä¸è¶…è¿‡å¯ç”¨æ–‡æ¡£æ•°é‡
        top_k = min(top_k, len(self.docs))
        
        # å…ˆè·å–æ’åºåçš„ç´¢å¼•
        sorted_indices = np.argsort(similarities)[::-1]
        
        # åˆ›å»ºç»“æœåˆ—è¡¨
        results = []
        
        # éå†æ’åºåçš„å‰top_kä¸ªæ–‡æ¡£
        for i in sorted_indices[:top_k]:
            # æ£€æŸ¥ç›¸ä¼¼åº¦æ˜¯å¦é«˜äºé˜ˆå€¼
            score = float(similarities[i])  # ç¡®ä¿è½¬æ¢ä¸ºPython floatç±»å‹
            if self.similarity_metric == "cosine" and score < threshold:
                # å¯¹äºä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå°äºé˜ˆå€¼åˆ™è·³è¿‡
                continue
            elif self.similarity_metric == "l2" and score < threshold:
                # å¯¹äºL2è·ç¦»ï¼ˆå·²å–è´Ÿå€¼ï¼‰ï¼Œå°äºé˜ˆå€¼åˆ™è·³è¿‡
                continue
                
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            results.append({
                "score": score,
                "content": self.docs[i]["content"],
                "file_path": self.docs[i]["file_path"]
            })
            
        return results
    
    def generate_prompt(self, query: str, top_k: int = None, threshold: float = 0.4) -> str:
        """
        ç”Ÿæˆ RAG æç¤º
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param top_k: è¿”å›æœ€ç›¸å…³çš„ top_k æ–‡æ¡£
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        :return: ç”Ÿæˆçš„æç¤ºæ–‡æœ¬
        """
        if top_k is None:
            top_k = self.top_k
        relevant_docs = self.retrieve_documents(query, top_k, threshold)
        prompt = f"""
        è¯·æ ¹æ®ç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜ã€‚è‹¥æœ‰çš„æ–‡æ¡£ä¸ç›¸å…³ï¼Œå°½é‡ä¸è¦è¾“å‡ºä¸ä¸ç›¸å…³æ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ ¹æ®ä½ è‡ªå·±æ¥è¾“å‡ºã€‚

        ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜: {query}

        ç›¸å…³æ–‡æ¡£:\n
        """
        for i, doc in enumerate(relevant_docs):
            prompt += f"""
            æ–‡æ¡£ {i+1} [æ¥è‡ª: {doc['file_path']}]:
            {doc['content']}
            ç›¸ä¼¼åº¦å¾—åˆ†: {doc['score']:.4f}\n\n
            """
        
        prompt += "è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä¸å­˜åœ¨äºæ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚"
        return prompt