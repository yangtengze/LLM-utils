from utils.document_loader import CSVLoader, MDLoader, PDFLoader, TXTLoader, DocxLoader
from utils.load_config import configs
from utils.base_func import parse_response
from typing import List, Dict
from pathlib import Path
import numpy as np
import requests
from tqdm import tqdm
from FlagEmbedding import FlagModel
import numpy as np
import json
import os
import time

class Rag:
    def __init__(self):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹
        - æ”¯æŒå¤šç§æ–‡æ¡£åŠ è½½å™¨
        - è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„æ–‡æ¡£å’Œå‘é‡
        - æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£
        """
        self.config = configs
        self.documents_path = Path(self.config['rag']['document_path'])
        self.files = self.get_all_files_in_directory()
        print('documents files:', self.files)
        self.vector_store_path = Path(self.config['rag']['vector_store']['index_path'])
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¶è‡ªåŠ¨åŠ è½½å·²æœ‰æ•°æ®
        self.docs = self._load_metadata()
        self.doc_vectors = self._load_vectors()

        self.device = self.config['rag']['embedding_model']['device']
        self.embedding_model = FlagModel(self.config['rag']['embedding_model']['path'], 
                  query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                  use_fp16=True,devices=self.device)

        self.top_k = self.config['rag']['retrieval']['top_k']
        self.stream = self.config['ollama']['stream']
        self.score_threshold = self.config['rag']['retrieval']['score_threshold']
        self.similarity_metric = self.config['rag']['vector_store']['similarity_metric']

        # self.reranker_model = 
    
    def get_all_files_in_directory(self) -> List[str]:
        directory_path = Path(self.documents_path)  # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ª Path å¯¹è±¡
        return [str(file) for file in directory_path.rglob('*') if file.is_file()]

    def _get_vector_path(self) -> Path:
        """è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "doc_vectors.npy"

    def _get_metadata_path(self) -> Path:
        """è·å–å…ƒæ•°æ®å­˜å‚¨è·¯å¾„"""
        return self.vector_store_path / "metadata.json"

    def _load_vectors(self) -> np.ndarray:
        """åŠ è½½å·²ä¿å­˜çš„å‘é‡"""
        vector_path = self._get_vector_path()
        if vector_path.exists():
            return np.load(vector_path)
        return None

    def _load_metadata(self) -> List[Dict]:
        """åŠ è½½å·²ä¿å­˜çš„å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path()
        if metadata_path.exists():
            with open(metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def _save_data(self):
        """ä¿å­˜å½“å‰æ•°æ®åˆ°ç£ç›˜"""
        # ä¿å­˜å‘é‡
        if self.doc_vectors is not None:
            np.save(self._get_vector_path(), self.doc_vectors)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(self._get_metadata_path(), "w", encoding="utf-8") as f:
            json.dump(self.docs, f, ensure_ascii=False, indent=2)
    
    def _cosine_similarity(self,query_vector,doc_vectors):
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        '''
        ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        cos(Î¸) = (AÂ·B) / (|A|Â·|B|)
        å…¶ä¸­,Aå’ŒBæ˜¯ä¸¤ä¸ªå‘é‡,AÂ·Bæ˜¯å®ƒä»¬çš„ç‚¹ç§¯,|A|å’Œ|B|æ˜¯å®ƒä»¬çš„æ¨¡ã€‚

        query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
        doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        '''
        scores = []
        for i in range(len(doc_vectors)):
            score = np.dot(query_vector, doc_vectors[i]) / (np.linalg.norm(query_vector) * np.linalg.norm(doc_vectors[i]))
            scores.append(score)
        return np.array(scores)
    
    def _l2_similarity(self,query_vector,doc_vectors):
        # è®¡ç®—L2ç›¸ä¼¼åº¦
        '''
        L2ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼:
        d(A,B) = sqrt(sum((A_i - B_i)^2))
        å…¶ä¸­,Aå’ŒBæ˜¯ä¸¤ä¸ªå‘é‡,A_iå’ŒB_iæ˜¯å®ƒä»¬çš„ç¬¬iä¸ªå…ƒç´ ã€‚

        query_vector: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
        doc_vectors: æ–‡æ¡£å‘é‡ï¼Œå½¢çŠ¶ä¸º (num_docs, embedding_dim)
        '''
        scores = []
        for i in range(len(doc_vectors)):
            score = np.linalg.norm(query_vector - doc_vectors[i])
            scores.append(score)
        return np.array(scores)
        
    def load_documents(self, file_paths: List[str]) -> None:
        """
        åŠ è½½å¤šç§æ ¼å¼çš„æ–‡æ¡£
        """
        # è½¬æ¢æ‰€æœ‰è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
        abs_paths = [str(Path(fp).absolute()) for fp in file_paths]
        
        # è¿‡æ»¤å·²åŠ è½½æ–‡ä»¶ï¼ˆä½¿ç”¨é›†åˆæé«˜æŸ¥è¯¢æ•ˆç‡ï¼‰
        existing_files = {doc["file_path"] for doc in self.docs}
        new_files = [fp for fp in abs_paths if fp not in existing_files]
        
        if not new_files:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²åŠ è½½è¿‡")
            return

        try:
            # å°†numpyæ•°ç»„è½¬ä¸ºåˆ—è¡¨ä¾¿äºè¿½åŠ 
            if self.doc_vectors is not None:
                vectors = self.doc_vectors.tolist()
            else:
                vectors = []

            with tqdm(new_files, desc="ğŸ“ æ€»ä½“è¿›åº¦") as global_pbar:
                for file_path in global_pbar:
                    global_pbar.set_postfix_str(f'å¤„ç†ä¸­: {Path(file_path).name}')
                    
                    try:
                        # è·å–æ–‡ä»¶åŠ è½½å™¨
                        if file_path.endswith('.csv'):
                            loader = CSVLoader()
                        elif file_path.endswith('.docx'):
                            loader = DocxLoader()
                        elif file_path.endswith('.md'):
                            loader = MDLoader()
                        elif file_path.endswith('.pdf'):
                            loader = PDFLoader()
                        elif (file_path.endswith('.txt') or (Path(file_path).suffix == '' and Path(file_path).is_file())):
                            loader = TXTLoader()
                        else:
                            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

                        # åŠ è½½æ–‡æ¡£å†…å®¹
                        chunks = loader.load(file_path)
                        
                        # å¤„ç†æ–‡æ¡£å—
                        file_vectors = []
                        for chunk in tqdm(chunks, desc=f"ğŸ“„ {Path(file_path).name}", leave=True):
                            # å­˜å‚¨å…ƒæ•°æ®
                            self.docs.append({
                                "file_path": file_path,
                                "content": str(chunk),
                                "timestamp": time.time()  # æ·»åŠ æ—¶é—´æˆ³ç”¨äºç‰ˆæœ¬æ§åˆ¶
                            })
                            # ç”Ÿæˆå‘é‡
                            file_vectors.append(self.embedding_model.encode(str(chunk)))
                        
                        # è¿½åŠ å‘é‡
                        vectors.extend(file_vectors)

                    except Exception as e:
                        print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                        continue

            # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„
            self.doc_vectors = np.array(vectors)
            
        finally:
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_data()

    def reset(self):
        """é‡ç½®æ‰€æœ‰å­˜å‚¨æ•°æ®"""
        self.doc_vectors = None
        self.docs = []
        if self._get_vector_path().exists():
            os.remove(self._get_vector_path())
        if self._get_metadata_path().exists():
            os.remove(self._get_metadata_path())
    
    def retrieve_documents(self, query: str, top_k: int = None) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶è¿”å›å¸¦è·¯å¾„çš„ç»“æœ
        :param query: æŸ¥è¯¢æ–‡æœ¬
        :param top_k: è¿”å›ç»“æœæ•°é‡
        :return: åŒ…å«è·¯å¾„å’Œå†…å®¹çš„æ–‡æ¡£åˆ—è¡¨
        """
        if top_k is None:
            top_k = self.top_k  # ä½¿ç”¨ç±»å±æ€§ top_k ä½œä¸ºé»˜è®¤å€¼
        if self.doc_vectors is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ–‡æ¡£")

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode(query).reshape(1, -1)
        # query_vector.shape: (1, 1024)
        # doc_vectors.shape: (num_docs, 1024)
        # è®¡ç®—ç›¸ä¼¼åº¦
        if self.similarity_metric == "cosine":
            similarities = self._cosine_similarity(query_vector, self.doc_vectors).flatten()
        elif self.similarity_metric == "l2":
            similarities = self._l2_similarity(query_vector, self.doc_vectors).flatten()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼: {self.similarity_metric}")
        
        # ç¡®ä¿ top_k ä¸è¶…è¿‡å¯ç”¨æ–‡æ¡£æ•°é‡
        top_k = min(top_k, len(self.docs))
        sorted_indices = np.argsort(similarities)[::-1][:top_k]

        return [{
            "score": similarities[i],
            "content": self.docs[i]["content"],
            "file_path": self.docs[i]["file_path"]
        } for i in sorted_indices]
    
    def generate_prompt(self, query: str, top_k: int = None) -> str:
        """
        ç”Ÿæˆ RAG æç¤º
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param top_k: è¿”å›æœ€ç›¸å…³çš„ top_k æ–‡æ¡£
        :return: ç”Ÿæˆçš„æç¤ºæ–‡æœ¬
        """
        if top_k is None:
            top_k = self.top_k
        relevant_docs = self.retrieve_documents(query, top_k)
        prompt = f"""
        è¯·æ ¹æ®ç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜ã€‚è‹¥æœ‰çš„æ–‡æ¡£ä¸ç›¸å…³ï¼Œå°½é‡ä¸è¦è¾“å‡ºä¸ä¸ç›¸å…³æ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ ¹æ®ä½ è‡ªå·±æ¥è¾“å‡ºã€‚

        ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜: {query}

        ç›¸å…³æ–‡æ¡£:\n
        """
        for i, doc in enumerate(relevant_docs):
            prompt += f"""
            æ–‡æ¡£ {i+1} [æ¥è‡ª: {doc['file_path']}]:
            {doc['content']}
            ç›¸ä¼¼åº¦å¾—åˆ†: {doc['score']:.4f}\n\n
            """
        return prompt
    
    def generate_response(self, query: str) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆå“åº”
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :return: ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        prompt = self.generate_prompt(query)
        # print("[DEBUG] ç”Ÿæˆçš„æç¤º:\n", prompt)
        # è°ƒç”¨ç”Ÿæˆæ¨¡å‹
        response = self._call_language_model(prompt)
        return response

    def _call_language_model(self, prompt: str) -> str:
        """
        è°ƒç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”
        :param prompt: æç¤ºæ–‡æœ¬
        :return: ç”Ÿæˆçš„å“åº”
        """
        self.llm_model = self.config['ollama']['default_model']
        data = {
            "model": self.llm_model,
            "prompt": prompt,
            "stream": self.stream,
            "options": {
                "temperature": self.config['ollama']['temperature']
            },
        }
        url = self.config['ollama']['endpoint'] + '/api/generate'
        
        try:
            response = requests.post(url, data=json.dumps(data))
            if response.status_code == 200:
                return parse_response(response, self.stream)
            else:
                error_msg = f"LLMè°ƒç”¨å¤±è´¥: HTTP {response.status_code}"
                print(error_msg)
                return error_msg
        except Exception as e:
            error_msg = f"LLMè°ƒç”¨å‡ºé”™: {str(e)}"
            print(error_msg)
            return error_msg

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    rag = Rag()
    # åŠ è½½æ–‡æ¡£
    rag.load_documents(rag.files)
    
    # ç”Ÿæˆå“åº”
    query = 'llama2å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é‡æœ‰å¤šå°‘çš„ï¼Ÿ'
    response = rag.generate_response(query)
    print(response)